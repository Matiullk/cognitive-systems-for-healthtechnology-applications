{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1. Heart Disease Classification\n",
    "# Ville Seeste 4.2.2018 \n",
    "# Helsinki Metropolia University of Applied Sciences\n",
    "\n",
    "Objectives in this exercise is to learn how to preprocess data and with that data try to diagnos heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras \n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "colnames = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'diagnosis']\n",
    "\n",
    "# Processed data\n",
    "\n",
    "pCleveland = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data', na_values='?', names=colnames)\n",
    "pHungarian = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data', na_values='?', names=colnames)\n",
    "pSwitzerland = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.switzerland.data', na_values='?', names=colnames)\n",
    "pLongBeachVA = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.va.data', na_values='?', names=colnames)\n",
    "\n",
    "# Raw data\n",
    "#\n",
    "# Reprocessed hungarian data seems to be same as processed\n",
    "# Raw Cleveland data seems to be broken\n",
    "#rHungarian = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/hungarian.data', na_values='-9', sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datas = [pCleveland, pHungarian, pSwitzerland, pLongBeachVA]\n",
    "data = pd.concat(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>861.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>830.000000</td>\n",
       "      <td>918.000000</td>\n",
       "      <td>865.000000</td>\n",
       "      <td>865.000000</td>\n",
       "      <td>858.000000</td>\n",
       "      <td>611.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>434.000000</td>\n",
       "      <td>920.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>53.510870</td>\n",
       "      <td>0.789130</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>132.132404</td>\n",
       "      <td>199.130337</td>\n",
       "      <td>0.166265</td>\n",
       "      <td>0.604575</td>\n",
       "      <td>137.545665</td>\n",
       "      <td>0.389595</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>1.770867</td>\n",
       "      <td>0.676375</td>\n",
       "      <td>5.087558</td>\n",
       "      <td>0.995652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.424685</td>\n",
       "      <td>0.408148</td>\n",
       "      <td>0.930969</td>\n",
       "      <td>19.066070</td>\n",
       "      <td>110.780810</td>\n",
       "      <td>0.372543</td>\n",
       "      <td>0.805827</td>\n",
       "      <td>25.926276</td>\n",
       "      <td>0.487941</td>\n",
       "      <td>1.091226</td>\n",
       "      <td>0.619256</td>\n",
       "      <td>0.935653</td>\n",
       "      <td>1.919075</td>\n",
       "      <td>1.142693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>603.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age         sex          cp    trestbps        chol         fbs  \\\n",
       "count  920.000000  920.000000  920.000000  861.000000  890.000000  830.000000   \n",
       "mean    53.510870    0.789130    3.250000  132.132404  199.130337    0.166265   \n",
       "std      9.424685    0.408148    0.930969   19.066070  110.780810    0.372543   \n",
       "min     28.000000    0.000000    1.000000    0.000000    0.000000    0.000000   \n",
       "25%     47.000000    1.000000    3.000000  120.000000  175.000000    0.000000   \n",
       "50%     54.000000    1.000000    4.000000  130.000000  223.000000    0.000000   \n",
       "75%     60.000000    1.000000    4.000000  140.000000  268.000000    0.000000   \n",
       "max     77.000000    1.000000    4.000000  200.000000  603.000000    1.000000   \n",
       "\n",
       "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
       "count  918.000000  865.000000  865.000000  858.000000  611.000000  309.000000   \n",
       "mean     0.604575  137.545665    0.389595    0.878788    1.770867    0.676375   \n",
       "std      0.805827   25.926276    0.487941    1.091226    0.619256    0.935653   \n",
       "min      0.000000   60.000000    0.000000   -2.600000    1.000000    0.000000   \n",
       "25%      0.000000  120.000000    0.000000    0.000000    1.000000    0.000000   \n",
       "50%      0.000000  140.000000    0.000000    0.500000    2.000000    0.000000   \n",
       "75%      1.000000  157.000000    1.000000    1.500000    2.000000    1.000000   \n",
       "max      2.000000  202.000000    1.000000    6.200000    3.000000    3.000000   \n",
       "\n",
       "             thal   diagnosis  \n",
       "count  434.000000  920.000000  \n",
       "mean     5.087558    0.995652  \n",
       "std      1.919075    1.142693  \n",
       "min      3.000000    0.000000  \n",
       "25%      3.000000    0.000000  \n",
       "50%      6.000000    1.000000  \n",
       "75%      7.000000    2.000000  \n",
       "max      7.000000    4.000000  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = shuffle(data)\n",
    "diagnoses = data['diagnosis']\n",
    "data = data.drop(['diagnosis'], axis=1)\n",
    "data = data.fillna(data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194    2\n",
       "126    1\n",
       "107    0\n",
       "45     4\n",
       "205    1\n",
       "279    1\n",
       "229    2\n",
       "273    0\n",
       "137    3\n",
       "68     0\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnoses.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = diagnoses[:644]\n",
    "y_test = diagnoses[644:]\n",
    "# to categorical\n",
    "#y_train = keras.utils.to_categorical(y_train)\n",
    "#y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "x_train = data[:644]\n",
    "x_test = data[644:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean = np.mean(x_train, axis=0)\n",
    "std = np.std(x_train, axis=0)\n",
    "\n",
    "x_train -= mean\n",
    "x_train /= (2 * std)\n",
    "\n",
    "x_test -= mean\n",
    "x_test /= (2 * std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 151\n",
      "Trainable params: 151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "age         float64\n",
      "sex         float64\n",
      "cp          float64\n",
      "trestbps    float64\n",
      "chol        float64\n",
      "fbs         float64\n",
      "restecg     float64\n",
      "thalach     float64\n",
      "exang       float64\n",
      "oldpeak     float64\n",
      "slope       float64\n",
      "ca          float64\n",
      "thal        float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(10, input_shape=(13,), activation='relu'))\n",
    "#model.add(keras.layers.Dense(10, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "print(x_test.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 644 samples, validate on 276 samples\n",
      "Epoch 1/1000\n",
      "644/644 [==============================] - 1s 2ms/step - loss: 0.3913 - acc: 0.4767 - val_loss: 0.4086 - val_acc: 0.5290\n",
      "Epoch 2/1000\n",
      "644/644 [==============================] - 0s 123us/step - loss: 0.3110 - acc: 0.4689 - val_loss: 0.3553 - val_acc: 0.5435\n",
      "Epoch 3/1000\n",
      "644/644 [==============================] - 0s 113us/step - loss: 0.2449 - acc: 0.4705 - val_loss: 0.3038 - val_acc: 0.5471\n",
      "Epoch 4/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: 0.1795 - acc: 0.4736 - val_loss: 0.2550 - val_acc: 0.5435\n",
      "Epoch 5/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: 0.1174 - acc: 0.4720 - val_loss: 0.2070 - val_acc: 0.5616\n",
      "Epoch 6/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: 0.0529 - acc: 0.4752 - val_loss: 0.1554 - val_acc: 0.5543\n",
      "Epoch 7/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -0.0112 - acc: 0.4736 - val_loss: 0.1069 - val_acc: 0.5580\n",
      "Epoch 8/1000\n",
      "644/644 [==============================] - 0s 113us/step - loss: -0.0748 - acc: 0.4783 - val_loss: 0.0565 - val_acc: 0.5616\n",
      "Epoch 9/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -0.1410 - acc: 0.4798 - val_loss: 0.0040 - val_acc: 0.5652\n",
      "Epoch 10/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -0.2078 - acc: 0.4845 - val_loss: -0.0479 - val_acc: 0.5652\n",
      "Epoch 11/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -0.2757 - acc: 0.4891 - val_loss: -0.1000 - val_acc: 0.5652\n",
      "Epoch 12/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -0.3458 - acc: 0.4891 - val_loss: -0.1534 - val_acc: 0.5652\n",
      "Epoch 13/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -0.4176 - acc: 0.4891 - val_loss: -0.2092 - val_acc: 0.5688\n",
      "Epoch 14/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -0.4901 - acc: 0.4891 - val_loss: -0.2642 - val_acc: 0.5725\n",
      "Epoch 15/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -0.5644 - acc: 0.4907 - val_loss: -0.3203 - val_acc: 0.5725\n",
      "Epoch 16/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -0.6400 - acc: 0.4953 - val_loss: -0.3763 - val_acc: 0.5725\n",
      "Epoch 17/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -0.7191 - acc: 0.4953 - val_loss: -0.4351 - val_acc: 0.5761\n",
      "Epoch 18/1000\n",
      "644/644 [==============================] - 0s 122us/step - loss: -0.7980 - acc: 0.4938 - val_loss: -0.4929 - val_acc: 0.5761\n",
      "Epoch 19/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -0.8796 - acc: 0.4938 - val_loss: -0.5547 - val_acc: 0.5797\n",
      "Epoch 20/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -0.9640 - acc: 0.4922 - val_loss: -0.6162 - val_acc: 0.5761\n",
      "Epoch 21/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -1.0471 - acc: 0.4922 - val_loss: -0.6766 - val_acc: 0.5797\n",
      "Epoch 22/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -1.1326 - acc: 0.4969 - val_loss: -0.7399 - val_acc: 0.5797\n",
      "Epoch 23/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -1.2212 - acc: 0.4953 - val_loss: -0.8040 - val_acc: 0.5833\n",
      "Epoch 24/1000\n",
      "644/644 [==============================] - 0s 119us/step - loss: -1.3092 - acc: 0.4953 - val_loss: -0.8683 - val_acc: 0.5833\n",
      "Epoch 25/1000\n",
      "644/644 [==============================] - 0s 130us/step - loss: -1.4012 - acc: 0.4953 - val_loss: -0.9362 - val_acc: 0.5833\n",
      "Epoch 26/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -1.4957 - acc: 0.4953 - val_loss: -1.0039 - val_acc: 0.5870\n",
      "Epoch 27/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -1.5925 - acc: 0.4953 - val_loss: -1.0740 - val_acc: 0.5833\n",
      "Epoch 28/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -1.6866 - acc: 0.4969 - val_loss: -1.1410 - val_acc: 0.5870\n",
      "Epoch 29/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -1.7838 - acc: 0.4969 - val_loss: -1.2099 - val_acc: 0.5870\n",
      "Epoch 30/1000\n",
      "644/644 [==============================] - 0s 117us/step - loss: -1.8818 - acc: 0.4969 - val_loss: -1.2803 - val_acc: 0.5797\n",
      "Epoch 31/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -1.9829 - acc: 0.4969 - val_loss: -1.3549 - val_acc: 0.5797\n",
      "Epoch 32/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -2.0848 - acc: 0.4984 - val_loss: -1.4283 - val_acc: 0.5761\n",
      "Epoch 33/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -2.1895 - acc: 0.4984 - val_loss: -1.5063 - val_acc: 0.5761\n",
      "Epoch 34/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -2.3017 - acc: 0.4984 - val_loss: -1.5867 - val_acc: 0.5761\n",
      "Epoch 35/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -2.4134 - acc: 0.4984 - val_loss: -1.6677 - val_acc: 0.5761\n",
      "Epoch 36/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -2.5237 - acc: 0.4984 - val_loss: -1.7478 - val_acc: 0.5761\n",
      "Epoch 37/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -2.6371 - acc: 0.4984 - val_loss: -1.8304 - val_acc: 0.5797\n",
      "Epoch 38/1000\n",
      "644/644 [==============================] - 0s 112us/step - loss: -2.7527 - acc: 0.4984 - val_loss: -1.9156 - val_acc: 0.5833\n",
      "Epoch 39/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -2.8695 - acc: 0.4969 - val_loss: -1.9982 - val_acc: 0.5833\n",
      "Epoch 40/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -2.9868 - acc: 0.4969 - val_loss: -2.0864 - val_acc: 0.5833\n",
      "Epoch 41/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -3.1076 - acc: 0.4984 - val_loss: -2.1731 - val_acc: 0.5833\n",
      "Epoch 42/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -3.2303 - acc: 0.4984 - val_loss: -2.2625 - val_acc: 0.5833\n",
      "Epoch 43/1000\n",
      "644/644 [==============================] - 0s 113us/step - loss: -3.3533 - acc: 0.4984 - val_loss: -2.3475 - val_acc: 0.5870\n",
      "Epoch 44/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -3.4772 - acc: 0.5000 - val_loss: -2.4312 - val_acc: 0.5870\n",
      "Epoch 45/1000\n",
      "644/644 [==============================] - 0s 112us/step - loss: -3.6040 - acc: 0.5000 - val_loss: -2.5237 - val_acc: 0.5870\n",
      "Epoch 46/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -3.7279 - acc: 0.5031 - val_loss: -2.6127 - val_acc: 0.5870\n",
      "Epoch 47/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -3.8564 - acc: 0.5016 - val_loss: -2.6971 - val_acc: 0.5870\n",
      "Epoch 48/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -3.9854 - acc: 0.5016 - val_loss: -2.7815 - val_acc: 0.5870\n",
      "Epoch 49/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -4.0950 - acc: 0.5031 - val_loss: -2.8536 - val_acc: 0.5870\n",
      "Epoch 50/1000\n",
      "644/644 [==============================] - ETA: 0s - loss: -5.2214 - acc: 0.50 - 0s 101us/step - loss: -4.1947 - acc: 0.5016 - val_loss: -2.9262 - val_acc: 0.5833\n",
      "Epoch 51/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -4.2846 - acc: 0.5031 - val_loss: -2.9933 - val_acc: 0.5833\n",
      "Epoch 52/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -4.3701 - acc: 0.5031 - val_loss: -3.0680 - val_acc: 0.5833\n",
      "Epoch 53/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -4.4513 - acc: 0.5031 - val_loss: -3.1276 - val_acc: 0.5833\n",
      "Epoch 54/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -4.5298 - acc: 0.5031 - val_loss: -3.1867 - val_acc: 0.5833\n",
      "Epoch 55/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -4.5986 - acc: 0.5047 - val_loss: -3.2493 - val_acc: 0.5833\n",
      "Epoch 56/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -4.6611 - acc: 0.5047 - val_loss: -3.3066 - val_acc: 0.5833\n",
      "Epoch 57/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -4.7208 - acc: 0.5047 - val_loss: -3.3496 - val_acc: 0.5833\n",
      "Epoch 58/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -4.7777 - acc: 0.5047 - val_loss: -3.3899 - val_acc: 0.5833\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644/644 [==============================] - 0s 100us/step - loss: -4.8297 - acc: 0.5062 - val_loss: -3.4203 - val_acc: 0.5833\n",
      "Epoch 60/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -4.8927 - acc: 0.5062 - val_loss: -3.4529 - val_acc: 0.5833\n",
      "Epoch 61/1000\n",
      "644/644 [==============================] - 0s 117us/step - loss: -4.9487 - acc: 0.5062 - val_loss: -3.4815 - val_acc: 0.5833\n",
      "Epoch 62/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -5.0075 - acc: 0.5062 - val_loss: -3.5063 - val_acc: 0.5833\n",
      "Epoch 63/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -5.0623 - acc: 0.5062 - val_loss: -3.5293 - val_acc: 0.5833\n",
      "Epoch 64/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.1190 - acc: 0.5062 - val_loss: -3.5536 - val_acc: 0.5833\n",
      "Epoch 65/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.1647 - acc: 0.5062 - val_loss: -3.5753 - val_acc: 0.5833\n",
      "Epoch 66/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -5.2031 - acc: 0.5062 - val_loss: -3.5951 - val_acc: 0.5833\n",
      "Epoch 67/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.2383 - acc: 0.5062 - val_loss: -3.6123 - val_acc: 0.5833\n",
      "Epoch 68/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.2763 - acc: 0.5062 - val_loss: -3.6275 - val_acc: 0.5833\n",
      "Epoch 69/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.3088 - acc: 0.5062 - val_loss: -3.6376 - val_acc: 0.5833\n",
      "Epoch 70/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -5.3410 - acc: 0.5062 - val_loss: -3.6571 - val_acc: 0.5833\n",
      "Epoch 71/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.3683 - acc: 0.5062 - val_loss: -3.6676 - val_acc: 0.5833\n",
      "Epoch 72/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -5.3911 - acc: 0.5062 - val_loss: -3.6791 - val_acc: 0.5833\n",
      "Epoch 73/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.4169 - acc: 0.5062 - val_loss: -3.6852 - val_acc: 0.5833\n",
      "Epoch 74/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -5.4341 - acc: 0.5062 - val_loss: -3.6952 - val_acc: 0.5833\n",
      "Epoch 75/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.4540 - acc: 0.5062 - val_loss: -3.6981 - val_acc: 0.5833\n",
      "Epoch 76/1000\n",
      "644/644 [==============================] - 0s 94us/step - loss: -5.4690 - acc: 0.5062 - val_loss: -3.7061 - val_acc: 0.5833\n",
      "Epoch 77/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -5.4849 - acc: 0.5078 - val_loss: -3.7126 - val_acc: 0.5833\n",
      "Epoch 78/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -5.4951 - acc: 0.5078 - val_loss: -3.7252 - val_acc: 0.5833\n",
      "Epoch 79/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.5041 - acc: 0.5078 - val_loss: -3.7278 - val_acc: 0.5833\n",
      "Epoch 80/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -5.5143 - acc: 0.5078 - val_loss: -3.7301 - val_acc: 0.5870\n",
      "Epoch 81/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.5218 - acc: 0.5062 - val_loss: -3.7408 - val_acc: 0.5870\n",
      "Epoch 82/1000\n",
      "644/644 [==============================] - 0s 95us/step - loss: -5.5296 - acc: 0.5078 - val_loss: -3.7420 - val_acc: 0.5870\n",
      "Epoch 83/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -5.5347 - acc: 0.5078 - val_loss: -3.7429 - val_acc: 0.5870\n",
      "Epoch 84/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.5414 - acc: 0.5078 - val_loss: -3.7498 - val_acc: 0.5870\n",
      "Epoch 85/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.5501 - acc: 0.5078 - val_loss: -3.7515 - val_acc: 0.5870\n",
      "Epoch 86/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -5.5544 - acc: 0.5078 - val_loss: -3.7515 - val_acc: 0.5870\n",
      "Epoch 87/1000\n",
      "644/644 [==============================] - 0s 121us/step - loss: -5.5568 - acc: 0.5078 - val_loss: -3.7532 - val_acc: 0.5870\n",
      "Epoch 88/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.5611 - acc: 0.5078 - val_loss: -3.7539 - val_acc: 0.5833\n",
      "Epoch 89/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.5716 - acc: 0.5078 - val_loss: -3.7544 - val_acc: 0.5833\n",
      "Epoch 90/1000\n",
      "644/644 [==============================] - 0s 94us/step - loss: -5.5790 - acc: 0.5062 - val_loss: -3.7608 - val_acc: 0.5833\n",
      "Epoch 91/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -5.5824 - acc: 0.5093 - val_loss: -3.7625 - val_acc: 0.5833\n",
      "Epoch 92/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.5887 - acc: 0.5109 - val_loss: -3.7631 - val_acc: 0.5833\n",
      "Epoch 93/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.5937 - acc: 0.5124 - val_loss: -3.7641 - val_acc: 0.5833\n",
      "Epoch 94/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -5.5971 - acc: 0.5124 - val_loss: -3.7635 - val_acc: 0.5833\n",
      "Epoch 95/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.6012 - acc: 0.5124 - val_loss: -3.7660 - val_acc: 0.5833\n",
      "Epoch 96/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.6052 - acc: 0.5140 - val_loss: -3.7655 - val_acc: 0.5833\n",
      "Epoch 97/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.6068 - acc: 0.5140 - val_loss: -3.7655 - val_acc: 0.5833\n",
      "Epoch 98/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.6124 - acc: 0.5140 - val_loss: -3.7678 - val_acc: 0.5833\n",
      "Epoch 99/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.6153 - acc: 0.5140 - val_loss: -3.7670 - val_acc: 0.5833\n",
      "Epoch 100/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -5.6178 - acc: 0.5140 - val_loss: -3.7660 - val_acc: 0.5833\n",
      "Epoch 101/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.6218 - acc: 0.5140 - val_loss: -3.7661 - val_acc: 0.5833\n",
      "Epoch 102/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.6242 - acc: 0.5171 - val_loss: -3.7670 - val_acc: 0.5833\n",
      "Epoch 103/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.6284 - acc: 0.5171 - val_loss: -3.7674 - val_acc: 0.5833\n",
      "Epoch 104/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -5.6326 - acc: 0.5171 - val_loss: -3.7657 - val_acc: 0.5870\n",
      "Epoch 105/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.6360 - acc: 0.5171 - val_loss: -3.7679 - val_acc: 0.5870\n",
      "Epoch 106/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -5.6390 - acc: 0.5186 - val_loss: -3.7661 - val_acc: 0.5870\n",
      "Epoch 107/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.6411 - acc: 0.5171 - val_loss: -3.7663 - val_acc: 0.5870\n",
      "Epoch 108/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -5.6444 - acc: 0.5186 - val_loss: -3.7703 - val_acc: 0.5870\n",
      "Epoch 109/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.6471 - acc: 0.5186 - val_loss: -3.7686 - val_acc: 0.5870\n",
      "Epoch 110/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.6503 - acc: 0.5186 - val_loss: -3.7678 - val_acc: 0.5870\n",
      "Epoch 111/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.6536 - acc: 0.5186 - val_loss: -3.7671 - val_acc: 0.5870\n",
      "Epoch 112/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -5.6558 - acc: 0.5186 - val_loss: -3.7661 - val_acc: 0.5870\n",
      "Epoch 113/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -5.6580 - acc: 0.5186 - val_loss: -3.7649 - val_acc: 0.5906\n",
      "Epoch 114/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -5.6612 - acc: 0.5202 - val_loss: -3.7642 - val_acc: 0.5906\n",
      "Epoch 115/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.6664 - acc: 0.5202 - val_loss: -3.7633 - val_acc: 0.5906\n",
      "Epoch 116/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.6688 - acc: 0.5202 - val_loss: -3.7618 - val_acc: 0.5906\n",
      "Epoch 117/1000\n",
      "644/644 [==============================] - 0s 95us/step - loss: -5.6703 - acc: 0.5217 - val_loss: -3.7608 - val_acc: 0.5942\n",
      "Epoch 118/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644/644 [==============================] - 0s 102us/step - loss: -5.6730 - acc: 0.5202 - val_loss: -3.7604 - val_acc: 0.5942\n",
      "Epoch 119/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -5.6757 - acc: 0.5202 - val_loss: -3.7588 - val_acc: 0.5942\n",
      "Epoch 120/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.6789 - acc: 0.5202 - val_loss: -3.7579 - val_acc: 0.5942\n",
      "Epoch 121/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.6843 - acc: 0.5202 - val_loss: -3.7585 - val_acc: 0.5942\n",
      "Epoch 122/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.6860 - acc: 0.5202 - val_loss: -3.7590 - val_acc: 0.5942\n",
      "Epoch 123/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.6908 - acc: 0.5202 - val_loss: -3.7598 - val_acc: 0.5978\n",
      "Epoch 124/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -5.6958 - acc: 0.5202 - val_loss: -3.7564 - val_acc: 0.5978\n",
      "Epoch 125/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -5.6964 - acc: 0.5217 - val_loss: -3.7555 - val_acc: 0.5978\n",
      "Epoch 126/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.6992 - acc: 0.5217 - val_loss: -3.7538 - val_acc: 0.5978\n",
      "Epoch 127/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -5.7005 - acc: 0.5202 - val_loss: -3.7517 - val_acc: 0.5978\n",
      "Epoch 128/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.7021 - acc: 0.5202 - val_loss: -3.7507 - val_acc: 0.5942\n",
      "Epoch 129/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.7073 - acc: 0.5217 - val_loss: -3.7491 - val_acc: 0.5942\n",
      "Epoch 130/1000\n",
      "644/644 [==============================] - 0s 118us/step - loss: -5.7099 - acc: 0.5217 - val_loss: -3.7504 - val_acc: 0.5942\n",
      "Epoch 131/1000\n",
      "644/644 [==============================] - 0s 114us/step - loss: -5.7131 - acc: 0.5217 - val_loss: -3.7496 - val_acc: 0.5942\n",
      "Epoch 132/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.7154 - acc: 0.5202 - val_loss: -3.7489 - val_acc: 0.5942\n",
      "Epoch 133/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -5.7190 - acc: 0.5217 - val_loss: -3.7502 - val_acc: 0.5942\n",
      "Epoch 134/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -5.7196 - acc: 0.5202 - val_loss: -3.7495 - val_acc: 0.5942\n",
      "Epoch 135/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.7238 - acc: 0.5202 - val_loss: -3.7488 - val_acc: 0.5942\n",
      "Epoch 136/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.7260 - acc: 0.5202 - val_loss: -3.7506 - val_acc: 0.5942\n",
      "Epoch 137/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.7274 - acc: 0.5186 - val_loss: -3.7486 - val_acc: 0.5942\n",
      "Epoch 138/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.7308 - acc: 0.5202 - val_loss: -3.7481 - val_acc: 0.5942\n",
      "Epoch 139/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -5.7329 - acc: 0.5202 - val_loss: -3.7478 - val_acc: 0.5942\n",
      "Epoch 140/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -5.7354 - acc: 0.5217 - val_loss: -3.7471 - val_acc: 0.5942\n",
      "Epoch 141/1000\n",
      "644/644 [==============================] - 0s 113us/step - loss: -5.7367 - acc: 0.5217 - val_loss: -3.7471 - val_acc: 0.5942\n",
      "Epoch 142/1000\n",
      "644/644 [==============================] - 0s 132us/step - loss: -5.7383 - acc: 0.5217 - val_loss: -3.7469 - val_acc: 0.5942\n",
      "Epoch 143/1000\n",
      "644/644 [==============================] - 0s 114us/step - loss: -5.7408 - acc: 0.5233 - val_loss: -3.7455 - val_acc: 0.5942\n",
      "Epoch 144/1000\n",
      "644/644 [==============================] - 0s 132us/step - loss: -5.7424 - acc: 0.5233 - val_loss: -3.7452 - val_acc: 0.5942\n",
      "Epoch 145/1000\n",
      "644/644 [==============================] - 0s 119us/step - loss: -5.7456 - acc: 0.5217 - val_loss: -3.7444 - val_acc: 0.5942\n",
      "Epoch 146/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -5.7447 - acc: 0.5233 - val_loss: -3.7433 - val_acc: 0.5906\n",
      "Epoch 147/1000\n",
      "644/644 [==============================] - 0s 124us/step - loss: -5.7473 - acc: 0.5217 - val_loss: -3.7436 - val_acc: 0.5906\n",
      "Epoch 148/1000\n",
      "644/644 [==============================] - 0s 120us/step - loss: -5.7492 - acc: 0.5233 - val_loss: -3.7406 - val_acc: 0.5906\n",
      "Epoch 149/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -5.7533 - acc: 0.5233 - val_loss: -3.7405 - val_acc: 0.5906\n",
      "Epoch 150/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -5.7525 - acc: 0.5217 - val_loss: -3.7400 - val_acc: 0.5906\n",
      "Epoch 151/1000\n",
      "644/644 [==============================] - 0s 127us/step - loss: -5.7557 - acc: 0.5233 - val_loss: -3.7398 - val_acc: 0.5906\n",
      "Epoch 152/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.7586 - acc: 0.5233 - val_loss: -3.7398 - val_acc: 0.5906\n",
      "Epoch 153/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -5.7604 - acc: 0.5264 - val_loss: -3.7397 - val_acc: 0.5906\n",
      "Epoch 154/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -5.7621 - acc: 0.5248 - val_loss: -3.7388 - val_acc: 0.5906\n",
      "Epoch 155/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.7632 - acc: 0.5248 - val_loss: -3.7372 - val_acc: 0.5906\n",
      "Epoch 156/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -5.7665 - acc: 0.5248 - val_loss: -3.7369 - val_acc: 0.5906\n",
      "Epoch 157/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -5.7684 - acc: 0.5248 - val_loss: -3.7369 - val_acc: 0.5870\n",
      "Epoch 158/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.7695 - acc: 0.5280 - val_loss: -3.7368 - val_acc: 0.5870\n",
      "Epoch 159/1000\n",
      "644/644 [==============================] - 0s 92us/step - loss: -5.7704 - acc: 0.5264 - val_loss: -3.7370 - val_acc: 0.5870\n",
      "Epoch 160/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -5.7735 - acc: 0.5264 - val_loss: -3.7362 - val_acc: 0.5870\n",
      "Epoch 161/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.7738 - acc: 0.5264 - val_loss: -3.7359 - val_acc: 0.5870\n",
      "Epoch 162/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -5.7767 - acc: 0.5264 - val_loss: -3.7361 - val_acc: 0.5870\n",
      "Epoch 163/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.7784 - acc: 0.5264 - val_loss: -3.7356 - val_acc: 0.5906\n",
      "Epoch 164/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -5.7807 - acc: 0.5264 - val_loss: -3.7350 - val_acc: 0.5906\n",
      "Epoch 165/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.7823 - acc: 0.5280 - val_loss: -3.7350 - val_acc: 0.5906\n",
      "Epoch 166/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -5.7836 - acc: 0.5280 - val_loss: -3.7348 - val_acc: 0.5906\n",
      "Epoch 167/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.7870 - acc: 0.5280 - val_loss: -3.7321 - val_acc: 0.5906\n",
      "Epoch 168/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.7875 - acc: 0.5280 - val_loss: -3.7324 - val_acc: 0.5906\n",
      "Epoch 169/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.7891 - acc: 0.5280 - val_loss: -3.7321 - val_acc: 0.5906\n",
      "Epoch 170/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.7898 - acc: 0.5280 - val_loss: -3.7321 - val_acc: 0.5906\n",
      "Epoch 171/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.7914 - acc: 0.5311 - val_loss: -3.7319 - val_acc: 0.5906\n",
      "Epoch 172/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -5.7933 - acc: 0.5311 - val_loss: -3.7321 - val_acc: 0.5906\n",
      "Epoch 173/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -5.7950 - acc: 0.5295 - val_loss: -3.7315 - val_acc: 0.5906\n",
      "Epoch 174/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -5.7973 - acc: 0.5311 - val_loss: -3.7306 - val_acc: 0.5906\n",
      "Epoch 175/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.7982 - acc: 0.5311 - val_loss: -3.7299 - val_acc: 0.5870\n",
      "Epoch 176/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -5.8028 - acc: 0.5311 - val_loss: -3.7298 - val_acc: 0.5870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.8043 - acc: 0.5311 - val_loss: -3.7290 - val_acc: 0.5870\n",
      "Epoch 178/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.8048 - acc: 0.5311 - val_loss: -3.7281 - val_acc: 0.5870\n",
      "Epoch 179/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.8063 - acc: 0.5311 - val_loss: -3.7282 - val_acc: 0.5870\n",
      "Epoch 180/1000\n",
      "644/644 [==============================] - 0s 95us/step - loss: -5.8089 - acc: 0.5326 - val_loss: -3.7284 - val_acc: 0.5870\n",
      "Epoch 181/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -5.8102 - acc: 0.5326 - val_loss: -3.7276 - val_acc: 0.5833\n",
      "Epoch 182/1000\n",
      "644/644 [==============================] - 0s 117us/step - loss: -5.8109 - acc: 0.5342 - val_loss: -3.7272 - val_acc: 0.5833\n",
      "Epoch 183/1000\n",
      "644/644 [==============================] - 0s 127us/step - loss: -5.8144 - acc: 0.5342 - val_loss: -3.7241 - val_acc: 0.5833\n",
      "Epoch 184/1000\n",
      "644/644 [==============================] - 0s 113us/step - loss: -5.8168 - acc: 0.5342 - val_loss: -3.7240 - val_acc: 0.5833\n",
      "Epoch 185/1000\n",
      "644/644 [==============================] - 0s 119us/step - loss: -5.8184 - acc: 0.5342 - val_loss: -3.7237 - val_acc: 0.5833\n",
      "Epoch 186/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -5.8191 - acc: 0.5342 - val_loss: -3.7233 - val_acc: 0.5833\n",
      "Epoch 187/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -5.8206 - acc: 0.5342 - val_loss: -3.7227 - val_acc: 0.5833\n",
      "Epoch 188/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -5.8215 - acc: 0.5342 - val_loss: -3.7223 - val_acc: 0.5833\n",
      "Epoch 189/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.8235 - acc: 0.5326 - val_loss: -3.7216 - val_acc: 0.5833\n",
      "Epoch 190/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.8254 - acc: 0.5342 - val_loss: -3.7210 - val_acc: 0.5833\n",
      "Epoch 191/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.8261 - acc: 0.5342 - val_loss: -3.7200 - val_acc: 0.5833\n",
      "Epoch 192/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -5.8277 - acc: 0.5342 - val_loss: -3.7198 - val_acc: 0.5833\n",
      "Epoch 193/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.8295 - acc: 0.5342 - val_loss: -3.7178 - val_acc: 0.5833\n",
      "Epoch 194/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.8285 - acc: 0.5326 - val_loss: -3.7176 - val_acc: 0.5833\n",
      "Epoch 195/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -5.8313 - acc: 0.5342 - val_loss: -3.7170 - val_acc: 0.5797\n",
      "Epoch 196/1000\n",
      "644/644 [==============================] - 0s 129us/step - loss: -5.8323 - acc: 0.5342 - val_loss: -3.7166 - val_acc: 0.5797\n",
      "Epoch 197/1000\n",
      "644/644 [==============================] - 0s 123us/step - loss: -5.8334 - acc: 0.5342 - val_loss: -3.7159 - val_acc: 0.5797\n",
      "Epoch 198/1000\n",
      "644/644 [==============================] - 0s 121us/step - loss: -5.8341 - acc: 0.5357 - val_loss: -3.7166 - val_acc: 0.5797\n",
      "Epoch 199/1000\n",
      "644/644 [==============================] - 0s 114us/step - loss: -5.8346 - acc: 0.5357 - val_loss: -3.7159 - val_acc: 0.5797\n",
      "Epoch 200/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.8367 - acc: 0.5357 - val_loss: -3.7151 - val_acc: 0.5797\n",
      "Epoch 201/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.8395 - acc: 0.5373 - val_loss: -3.7151 - val_acc: 0.5797\n",
      "Epoch 202/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.8389 - acc: 0.5373 - val_loss: -3.7140 - val_acc: 0.5797\n",
      "Epoch 203/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.8391 - acc: 0.5373 - val_loss: -3.7133 - val_acc: 0.5797\n",
      "Epoch 204/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.8412 - acc: 0.5357 - val_loss: -3.7119 - val_acc: 0.5797\n",
      "Epoch 205/1000\n",
      "644/644 [==============================] - 0s 112us/step - loss: -5.8423 - acc: 0.5357 - val_loss: -3.7113 - val_acc: 0.5797\n",
      "Epoch 206/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.8454 - acc: 0.5357 - val_loss: -3.7105 - val_acc: 0.5797\n",
      "Epoch 207/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -5.8467 - acc: 0.5357 - val_loss: -3.7097 - val_acc: 0.5797\n",
      "Epoch 208/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -5.8467 - acc: 0.5373 - val_loss: -3.7101 - val_acc: 0.5797\n",
      "Epoch 209/1000\n",
      "644/644 [==============================] - 0s 112us/step - loss: -5.8485 - acc: 0.5373 - val_loss: -3.7096 - val_acc: 0.5797\n",
      "Epoch 210/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.8496 - acc: 0.5373 - val_loss: -3.7090 - val_acc: 0.5797\n",
      "Epoch 211/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.8508 - acc: 0.5373 - val_loss: -3.7076 - val_acc: 0.5797\n",
      "Epoch 212/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -5.8520 - acc: 0.5373 - val_loss: -3.7065 - val_acc: 0.5797\n",
      "Epoch 213/1000\n",
      "644/644 [==============================] - 0s 113us/step - loss: -5.8531 - acc: 0.5373 - val_loss: -3.7055 - val_acc: 0.5797\n",
      "Epoch 214/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -5.8538 - acc: 0.5373 - val_loss: -3.7053 - val_acc: 0.5797\n",
      "Epoch 215/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.8551 - acc: 0.5373 - val_loss: -3.7043 - val_acc: 0.5797\n",
      "Epoch 216/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -5.8561 - acc: 0.5373 - val_loss: -3.7043 - val_acc: 0.5797\n",
      "Epoch 217/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -5.8560 - acc: 0.5373 - val_loss: -3.7048 - val_acc: 0.5797\n",
      "Epoch 218/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -5.8576 - acc: 0.5373 - val_loss: -3.7032 - val_acc: 0.5797\n",
      "Epoch 219/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.8594 - acc: 0.5388 - val_loss: -3.7023 - val_acc: 0.5797\n",
      "Epoch 220/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.8585 - acc: 0.5388 - val_loss: -3.7009 - val_acc: 0.5797\n",
      "Epoch 221/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.8606 - acc: 0.5388 - val_loss: -3.6997 - val_acc: 0.5797\n",
      "Epoch 222/1000\n",
      "644/644 [==============================] - 0s 113us/step - loss: -5.8607 - acc: 0.5388 - val_loss: -3.7002 - val_acc: 0.5797\n",
      "Epoch 223/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -5.8624 - acc: 0.5388 - val_loss: -3.6975 - val_acc: 0.5797\n",
      "Epoch 224/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.8621 - acc: 0.5388 - val_loss: -3.6977 - val_acc: 0.5797\n",
      "Epoch 225/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.8646 - acc: 0.5388 - val_loss: -3.6954 - val_acc: 0.5797\n",
      "Epoch 226/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.8655 - acc: 0.5388 - val_loss: -3.6955 - val_acc: 0.5797\n",
      "Epoch 227/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -5.8662 - acc: 0.5388 - val_loss: -3.6943 - val_acc: 0.5797\n",
      "Epoch 228/1000\n",
      "644/644 [==============================] - 0s 114us/step - loss: -5.8672 - acc: 0.5388 - val_loss: -3.6942 - val_acc: 0.5797\n",
      "Epoch 229/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -5.8679 - acc: 0.5388 - val_loss: -3.6933 - val_acc: 0.5797\n",
      "Epoch 230/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.8693 - acc: 0.5388 - val_loss: -3.6925 - val_acc: 0.5833\n",
      "Epoch 231/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.8702 - acc: 0.5404 - val_loss: -3.6915 - val_acc: 0.5833\n",
      "Epoch 232/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.8711 - acc: 0.5388 - val_loss: -3.6881 - val_acc: 0.5833\n",
      "Epoch 233/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.8731 - acc: 0.5388 - val_loss: -3.6873 - val_acc: 0.5833\n",
      "Epoch 234/1000\n",
      "644/644 [==============================] - 0s 119us/step - loss: -5.8743 - acc: 0.5388 - val_loss: -3.6866 - val_acc: 0.5833\n",
      "Epoch 235/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644/644 [==============================] - 0s 102us/step - loss: -5.8755 - acc: 0.5388 - val_loss: -3.6857 - val_acc: 0.5833\n",
      "Epoch 236/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.8758 - acc: 0.5388 - val_loss: -3.6852 - val_acc: 0.5833\n",
      "Epoch 237/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.8775 - acc: 0.5404 - val_loss: -3.6842 - val_acc: 0.5833\n",
      "Epoch 238/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.8791 - acc: 0.5404 - val_loss: -3.6860 - val_acc: 0.5833\n",
      "Epoch 239/1000\n",
      "644/644 [==============================] - 0s 112us/step - loss: -5.8781 - acc: 0.5404 - val_loss: -3.6856 - val_acc: 0.5833\n",
      "Epoch 240/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -5.8798 - acc: 0.5419 - val_loss: -3.6851 - val_acc: 0.5833\n",
      "Epoch 241/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -5.8808 - acc: 0.5419 - val_loss: -3.6850 - val_acc: 0.5833\n",
      "Epoch 242/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -5.8808 - acc: 0.5419 - val_loss: -3.6840 - val_acc: 0.5833\n",
      "Epoch 243/1000\n",
      "644/644 [==============================] - 0s 114us/step - loss: -5.8836 - acc: 0.5419 - val_loss: -3.6821 - val_acc: 0.5833\n",
      "Epoch 244/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.8834 - acc: 0.5419 - val_loss: -3.6817 - val_acc: 0.5833\n",
      "Epoch 245/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -5.8847 - acc: 0.5419 - val_loss: -3.6812 - val_acc: 0.5833\n",
      "Epoch 246/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.8845 - acc: 0.5419 - val_loss: -3.6807 - val_acc: 0.5833\n",
      "Epoch 247/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -5.8885 - acc: 0.5419 - val_loss: -3.6793 - val_acc: 0.5833\n",
      "Epoch 248/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.8897 - acc: 0.5435 - val_loss: -3.6785 - val_acc: 0.5833\n",
      "Epoch 249/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.8895 - acc: 0.5435 - val_loss: -3.6785 - val_acc: 0.5833\n",
      "Epoch 250/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.8907 - acc: 0.5435 - val_loss: -3.6778 - val_acc: 0.5833\n",
      "Epoch 251/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -5.8913 - acc: 0.5435 - val_loss: -3.6772 - val_acc: 0.5833\n",
      "Epoch 252/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.8927 - acc: 0.5435 - val_loss: -3.6773 - val_acc: 0.5833\n",
      "Epoch 253/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.8946 - acc: 0.5435 - val_loss: -3.6766 - val_acc: 0.5833\n",
      "Epoch 254/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.8943 - acc: 0.5435 - val_loss: -3.6764 - val_acc: 0.5833\n",
      "Epoch 255/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -5.8964 - acc: 0.5435 - val_loss: -3.6757 - val_acc: 0.5833\n",
      "Epoch 256/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.8972 - acc: 0.5450 - val_loss: -3.6753 - val_acc: 0.5833\n",
      "Epoch 257/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -5.8972 - acc: 0.5435 - val_loss: -3.6744 - val_acc: 0.5833\n",
      "Epoch 258/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.8988 - acc: 0.5450 - val_loss: -3.6709 - val_acc: 0.5797\n",
      "Epoch 259/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -5.8994 - acc: 0.5466 - val_loss: -3.6711 - val_acc: 0.5797\n",
      "Epoch 260/1000\n",
      "644/644 [==============================] - 0s 130us/step - loss: -5.8999 - acc: 0.5450 - val_loss: -3.6705 - val_acc: 0.5797\n",
      "Epoch 261/1000\n",
      "644/644 [==============================] - 0s 114us/step - loss: -5.9013 - acc: 0.5450 - val_loss: -3.6698 - val_acc: 0.5797\n",
      "Epoch 262/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -5.9020 - acc: 0.5450 - val_loss: -3.6666 - val_acc: 0.5797\n",
      "Epoch 263/1000\n",
      "644/644 [==============================] - 0s 130us/step - loss: -5.9012 - acc: 0.5466 - val_loss: -3.6670 - val_acc: 0.5797\n",
      "Epoch 264/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -5.9024 - acc: 0.5466 - val_loss: -3.6662 - val_acc: 0.5797\n",
      "Epoch 265/1000\n",
      "644/644 [==============================] - 0s 127us/step - loss: -5.9019 - acc: 0.5466 - val_loss: -3.6679 - val_acc: 0.5797\n",
      "Epoch 266/1000\n",
      "644/644 [==============================] - 0s 117us/step - loss: -5.9045 - acc: 0.5466 - val_loss: -3.6673 - val_acc: 0.5797\n",
      "Epoch 267/1000\n",
      "644/644 [==============================] - 0s 129us/step - loss: -5.9045 - acc: 0.5466 - val_loss: -3.6638 - val_acc: 0.5797\n",
      "Epoch 268/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -5.9055 - acc: 0.5466 - val_loss: -3.6613 - val_acc: 0.5797\n",
      "Epoch 269/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -5.9066 - acc: 0.5466 - val_loss: -3.6605 - val_acc: 0.5833\n",
      "Epoch 270/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -5.9066 - acc: 0.5466 - val_loss: -3.6602 - val_acc: 0.5797\n",
      "Epoch 271/1000\n",
      "644/644 [==============================] - 0s 112us/step - loss: -5.9075 - acc: 0.5466 - val_loss: -3.6599 - val_acc: 0.5761\n",
      "Epoch 272/1000\n",
      "644/644 [==============================] - 0s 112us/step - loss: -5.9078 - acc: 0.5466 - val_loss: -3.6640 - val_acc: 0.5761\n",
      "Epoch 273/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -5.9096 - acc: 0.5450 - val_loss: -3.6584 - val_acc: 0.5761\n",
      "Epoch 274/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -5.9103 - acc: 0.5450 - val_loss: -3.6586 - val_acc: 0.5761\n",
      "Epoch 275/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -5.9091 - acc: 0.5450 - val_loss: -3.6578 - val_acc: 0.5761\n",
      "Epoch 276/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -5.9107 - acc: 0.5450 - val_loss: -3.6572 - val_acc: 0.5761\n",
      "Epoch 277/1000\n",
      "644/644 [==============================] - 0s 113us/step - loss: -5.9120 - acc: 0.5466 - val_loss: -3.6574 - val_acc: 0.5797\n",
      "Epoch 278/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -5.9114 - acc: 0.5450 - val_loss: -3.6567 - val_acc: 0.5797\n",
      "Epoch 279/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -5.9121 - acc: 0.5466 - val_loss: -3.6568 - val_acc: 0.5797\n",
      "Epoch 280/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.9132 - acc: 0.5450 - val_loss: -3.6564 - val_acc: 0.5797\n",
      "Epoch 281/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.9143 - acc: 0.5466 - val_loss: -3.6571 - val_acc: 0.5797\n",
      "Epoch 282/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.9145 - acc: 0.5450 - val_loss: -3.6559 - val_acc: 0.5797\n",
      "Epoch 283/1000\n",
      "644/644 [==============================] - 0s 122us/step - loss: -5.9164 - acc: 0.5450 - val_loss: -3.6569 - val_acc: 0.5797\n",
      "Epoch 284/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.9162 - acc: 0.5450 - val_loss: -3.6560 - val_acc: 0.5797\n",
      "Epoch 285/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.9171 - acc: 0.5450 - val_loss: -3.6559 - val_acc: 0.5797\n",
      "Epoch 286/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.9183 - acc: 0.5466 - val_loss: -3.6550 - val_acc: 0.5797\n",
      "Epoch 287/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -5.9184 - acc: 0.5450 - val_loss: -3.6552 - val_acc: 0.5797\n",
      "Epoch 288/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -5.9200 - acc: 0.5450 - val_loss: -3.6549 - val_acc: 0.5797\n",
      "Epoch 289/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.9204 - acc: 0.5466 - val_loss: -3.6546 - val_acc: 0.5797\n",
      "Epoch 290/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.9217 - acc: 0.5481 - val_loss: -3.6538 - val_acc: 0.5797\n",
      "Epoch 291/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.9217 - acc: 0.5481 - val_loss: -3.6532 - val_acc: 0.5797\n",
      "Epoch 292/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -5.9227 - acc: 0.5481 - val_loss: -3.6536 - val_acc: 0.5797\n",
      "Epoch 293/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644/644 [==============================] - 0s 103us/step - loss: -5.9243 - acc: 0.5481 - val_loss: -3.6534 - val_acc: 0.5797\n",
      "Epoch 294/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.9245 - acc: 0.5481 - val_loss: -3.6527 - val_acc: 0.5797\n",
      "Epoch 295/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -5.9253 - acc: 0.5466 - val_loss: -3.6524 - val_acc: 0.5797\n",
      "Epoch 296/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.9266 - acc: 0.5466 - val_loss: -3.6528 - val_acc: 0.5797\n",
      "Epoch 297/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.9276 - acc: 0.5481 - val_loss: -3.6517 - val_acc: 0.5797\n",
      "Epoch 298/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -5.9277 - acc: 0.5497 - val_loss: -3.6517 - val_acc: 0.5797\n",
      "Epoch 299/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.9286 - acc: 0.5481 - val_loss: -3.6524 - val_acc: 0.5761\n",
      "Epoch 300/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -5.9290 - acc: 0.5497 - val_loss: -3.6515 - val_acc: 0.5761\n",
      "Epoch 301/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.9300 - acc: 0.5481 - val_loss: -3.6510 - val_acc: 0.5761\n",
      "Epoch 302/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -5.9293 - acc: 0.5481 - val_loss: -3.6502 - val_acc: 0.5761\n",
      "Epoch 303/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.9308 - acc: 0.5481 - val_loss: -3.6499 - val_acc: 0.5761\n",
      "Epoch 304/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.9313 - acc: 0.5497 - val_loss: -3.6496 - val_acc: 0.5761\n",
      "Epoch 305/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -5.9315 - acc: 0.5497 - val_loss: -3.6487 - val_acc: 0.5761\n",
      "Epoch 306/1000\n",
      "644/644 [==============================] - 0s 115us/step - loss: -5.9322 - acc: 0.5481 - val_loss: -3.6486 - val_acc: 0.5761\n",
      "Epoch 307/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -5.9323 - acc: 0.5497 - val_loss: -3.6491 - val_acc: 0.5761\n",
      "Epoch 308/1000\n",
      "644/644 [==============================] - 0s 112us/step - loss: -5.9332 - acc: 0.5497 - val_loss: -3.6483 - val_acc: 0.5761\n",
      "Epoch 309/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -5.9342 - acc: 0.5481 - val_loss: -3.6482 - val_acc: 0.5761\n",
      "Epoch 310/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.9352 - acc: 0.5481 - val_loss: -3.6478 - val_acc: 0.5761\n",
      "Epoch 311/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -5.9357 - acc: 0.5481 - val_loss: -3.6478 - val_acc: 0.5761\n",
      "Epoch 312/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -5.9364 - acc: 0.5481 - val_loss: -3.6470 - val_acc: 0.5761\n",
      "Epoch 313/1000\n",
      "644/644 [==============================] - 0s 113us/step - loss: -5.9376 - acc: 0.5481 - val_loss: -3.6468 - val_acc: 0.5761\n",
      "Epoch 314/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.9376 - acc: 0.5481 - val_loss: -3.6465 - val_acc: 0.5761\n",
      "Epoch 315/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.9378 - acc: 0.5481 - val_loss: -3.6454 - val_acc: 0.5761\n",
      "Epoch 316/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.9396 - acc: 0.5481 - val_loss: -3.6452 - val_acc: 0.5761\n",
      "Epoch 317/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -5.9412 - acc: 0.5481 - val_loss: -3.6453 - val_acc: 0.5761\n",
      "Epoch 318/1000\n",
      "644/644 [==============================] - 0s 117us/step - loss: -5.9425 - acc: 0.5481 - val_loss: -3.6449 - val_acc: 0.5761\n",
      "Epoch 319/1000\n",
      "644/644 [==============================] - 0s 121us/step - loss: -5.9424 - acc: 0.5481 - val_loss: -3.6448 - val_acc: 0.5761\n",
      "Epoch 320/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -5.9430 - acc: 0.5481 - val_loss: -3.6444 - val_acc: 0.5761\n",
      "Epoch 321/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -5.9435 - acc: 0.5481 - val_loss: -3.6435 - val_acc: 0.5761\n",
      "Epoch 322/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -5.9457 - acc: 0.5481 - val_loss: -3.6444 - val_acc: 0.5761\n",
      "Epoch 323/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -5.9409 - acc: 0.5481 - val_loss: -3.6436 - val_acc: 0.5761\n",
      "Epoch 324/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.9463 - acc: 0.5481 - val_loss: -3.6436 - val_acc: 0.5761\n",
      "Epoch 325/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.9467 - acc: 0.5481 - val_loss: -3.6434 - val_acc: 0.5761\n",
      "Epoch 326/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -5.9481 - acc: 0.5481 - val_loss: -3.6435 - val_acc: 0.5761\n",
      "Epoch 327/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.9463 - acc: 0.5481 - val_loss: -3.6428 - val_acc: 0.5797\n",
      "Epoch 328/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.9487 - acc: 0.5481 - val_loss: -3.6425 - val_acc: 0.5797\n",
      "Epoch 329/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -5.9468 - acc: 0.5466 - val_loss: -3.6420 - val_acc: 0.5797\n",
      "Epoch 330/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -5.9500 - acc: 0.5481 - val_loss: -3.6414 - val_acc: 0.5797\n",
      "Epoch 331/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.9505 - acc: 0.5466 - val_loss: -3.6408 - val_acc: 0.5797\n",
      "Epoch 332/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.9514 - acc: 0.5466 - val_loss: -3.6399 - val_acc: 0.5797\n",
      "Epoch 333/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.9526 - acc: 0.5466 - val_loss: -3.6398 - val_acc: 0.5797\n",
      "Epoch 334/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.9535 - acc: 0.5466 - val_loss: -3.6395 - val_acc: 0.5797\n",
      "Epoch 335/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -5.9502 - acc: 0.5466 - val_loss: -3.6390 - val_acc: 0.5797\n",
      "Epoch 336/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.9537 - acc: 0.5450 - val_loss: -3.6377 - val_acc: 0.5797\n",
      "Epoch 337/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -5.9538 - acc: 0.5450 - val_loss: -3.6374 - val_acc: 0.5797\n",
      "Epoch 338/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.9553 - acc: 0.5450 - val_loss: -3.6387 - val_acc: 0.5797\n",
      "Epoch 339/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.9530 - acc: 0.5450 - val_loss: -3.6372 - val_acc: 0.5797\n",
      "Epoch 340/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.9569 - acc: 0.5450 - val_loss: -3.6386 - val_acc: 0.5797\n",
      "Epoch 341/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -5.9579 - acc: 0.5450 - val_loss: -3.6378 - val_acc: 0.5797\n",
      "Epoch 342/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.9561 - acc: 0.5450 - val_loss: -3.6371 - val_acc: 0.5797\n",
      "Epoch 343/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.9589 - acc: 0.5450 - val_loss: -3.6360 - val_acc: 0.5797\n",
      "Epoch 344/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.9600 - acc: 0.5466 - val_loss: -3.6356 - val_acc: 0.5797\n",
      "Epoch 345/1000\n",
      "644/644 [==============================] - 0s 95us/step - loss: -5.9608 - acc: 0.5450 - val_loss: -3.6353 - val_acc: 0.5797\n",
      "Epoch 346/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -5.9612 - acc: 0.5450 - val_loss: -3.6349 - val_acc: 0.5797\n",
      "Epoch 347/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -5.9626 - acc: 0.5435 - val_loss: -3.6344 - val_acc: 0.5797\n",
      "Epoch 348/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.9624 - acc: 0.5450 - val_loss: -3.6337 - val_acc: 0.5797\n",
      "Epoch 349/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.9599 - acc: 0.5450 - val_loss: -3.6330 - val_acc: 0.5797\n",
      "Epoch 350/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -5.9646 - acc: 0.5450 - val_loss: -3.6329 - val_acc: 0.5797\n",
      "Epoch 351/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644/644 [==============================] - 0s 102us/step - loss: -5.9643 - acc: 0.5450 - val_loss: -3.6311 - val_acc: 0.5797\n",
      "Epoch 352/1000\n",
      "644/644 [==============================] - ETA: 0s - loss: -8.7104 - acc: 0.48 - 0s 101us/step - loss: -5.9663 - acc: 0.5450 - val_loss: -3.6308 - val_acc: 0.5797\n",
      "Epoch 353/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.9661 - acc: 0.5450 - val_loss: -3.6302 - val_acc: 0.5797\n",
      "Epoch 354/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -5.9642 - acc: 0.5450 - val_loss: -3.6301 - val_acc: 0.5797\n",
      "Epoch 355/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -5.9672 - acc: 0.5450 - val_loss: -3.6297 - val_acc: 0.5797\n",
      "Epoch 356/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.9687 - acc: 0.5466 - val_loss: -3.6306 - val_acc: 0.5797\n",
      "Epoch 357/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -5.9659 - acc: 0.5466 - val_loss: -3.6303 - val_acc: 0.5761\n",
      "Epoch 358/1000\n",
      "644/644 [==============================] - 0s 120us/step - loss: -5.9669 - acc: 0.5466 - val_loss: -3.6300 - val_acc: 0.5761\n",
      "Epoch 359/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -5.9712 - acc: 0.5497 - val_loss: -3.6295 - val_acc: 0.5761\n",
      "Epoch 360/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -5.9686 - acc: 0.5481 - val_loss: -3.6276 - val_acc: 0.5761\n",
      "Epoch 361/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.9707 - acc: 0.5497 - val_loss: -3.6268 - val_acc: 0.5761\n",
      "Epoch 362/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.9699 - acc: 0.5497 - val_loss: -3.6263 - val_acc: 0.5761\n",
      "Epoch 363/1000\n",
      "644/644 [==============================] - 0s 121us/step - loss: -5.9701 - acc: 0.5497 - val_loss: -3.6259 - val_acc: 0.5761\n",
      "Epoch 364/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -5.9707 - acc: 0.5497 - val_loss: -3.6262 - val_acc: 0.5761\n",
      "Epoch 365/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -5.9726 - acc: 0.5481 - val_loss: -3.6258 - val_acc: 0.5761\n",
      "Epoch 366/1000\n",
      "644/644 [==============================] - 0s 114us/step - loss: -5.9727 - acc: 0.5497 - val_loss: -3.6258 - val_acc: 0.5797\n",
      "Epoch 367/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.9721 - acc: 0.5497 - val_loss: -3.6256 - val_acc: 0.5761\n",
      "Epoch 368/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -5.9734 - acc: 0.5497 - val_loss: -3.6247 - val_acc: 0.5761\n",
      "Epoch 369/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.9732 - acc: 0.5497 - val_loss: -3.6247 - val_acc: 0.5761\n",
      "Epoch 370/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.9749 - acc: 0.5497 - val_loss: -3.6249 - val_acc: 0.5761\n",
      "Epoch 371/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.9761 - acc: 0.5481 - val_loss: -3.6257 - val_acc: 0.5761\n",
      "Epoch 372/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.9767 - acc: 0.5528 - val_loss: -3.6251 - val_acc: 0.5761\n",
      "Epoch 373/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -5.9769 - acc: 0.5497 - val_loss: -3.6254 - val_acc: 0.5761\n",
      "Epoch 374/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -5.9779 - acc: 0.5512 - val_loss: -3.6248 - val_acc: 0.5761\n",
      "Epoch 375/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.9779 - acc: 0.5497 - val_loss: -3.6253 - val_acc: 0.5761\n",
      "Epoch 376/1000\n",
      "644/644 [==============================] - 0s 120us/step - loss: -5.9779 - acc: 0.5481 - val_loss: -3.6248 - val_acc: 0.5761\n",
      "Epoch 377/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -5.9793 - acc: 0.5497 - val_loss: -3.6242 - val_acc: 0.5761\n",
      "Epoch 378/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -5.9789 - acc: 0.5481 - val_loss: -3.6213 - val_acc: 0.5761\n",
      "Epoch 379/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -5.9808 - acc: 0.5481 - val_loss: -3.6211 - val_acc: 0.5761\n",
      "Epoch 380/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.9815 - acc: 0.5512 - val_loss: -3.6205 - val_acc: 0.5761\n",
      "Epoch 381/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -5.9816 - acc: 0.5481 - val_loss: -3.6221 - val_acc: 0.5761\n",
      "Epoch 382/1000\n",
      "644/644 [==============================] - 0s 113us/step - loss: -5.9825 - acc: 0.5528 - val_loss: -3.6200 - val_acc: 0.5761\n",
      "Epoch 383/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -5.9823 - acc: 0.5481 - val_loss: -3.6215 - val_acc: 0.5761\n",
      "Epoch 384/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -5.9847 - acc: 0.5497 - val_loss: -3.6216 - val_acc: 0.5761\n",
      "Epoch 385/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.9875 - acc: 0.5528 - val_loss: -3.6216 - val_acc: 0.5761\n",
      "Epoch 386/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -5.9851 - acc: 0.5512 - val_loss: -3.6207 - val_acc: 0.5761\n",
      "Epoch 387/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -5.9865 - acc: 0.5512 - val_loss: -3.6197 - val_acc: 0.5761\n",
      "Epoch 388/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -5.9877 - acc: 0.5528 - val_loss: -3.6196 - val_acc: 0.5761\n",
      "Epoch 389/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -5.9906 - acc: 0.5528 - val_loss: -3.6200 - val_acc: 0.5761\n",
      "Epoch 390/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -5.9908 - acc: 0.5528 - val_loss: -3.6198 - val_acc: 0.5761\n",
      "Epoch 391/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.9932 - acc: 0.5528 - val_loss: -3.6194 - val_acc: 0.5761\n",
      "Epoch 392/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -5.9925 - acc: 0.5528 - val_loss: -3.6185 - val_acc: 0.5761\n",
      "Epoch 393/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -5.9936 - acc: 0.5512 - val_loss: -3.6181 - val_acc: 0.5761\n",
      "Epoch 394/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -5.9942 - acc: 0.5497 - val_loss: -3.6183 - val_acc: 0.5761\n",
      "Epoch 395/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -5.9926 - acc: 0.5528 - val_loss: -3.6177 - val_acc: 0.5761\n",
      "Epoch 396/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.9965 - acc: 0.5528 - val_loss: -3.6180 - val_acc: 0.5761\n",
      "Epoch 397/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -5.9970 - acc: 0.5512 - val_loss: -3.6177 - val_acc: 0.5761\n",
      "Epoch 398/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -5.9938 - acc: 0.5528 - val_loss: -3.6172 - val_acc: 0.5761\n",
      "Epoch 399/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -5.9991 - acc: 0.5528 - val_loss: -3.6176 - val_acc: 0.5761\n",
      "Epoch 400/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -5.9980 - acc: 0.5512 - val_loss: -3.6178 - val_acc: 0.5761\n",
      "Epoch 401/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -5.9990 - acc: 0.5512 - val_loss: -3.6169 - val_acc: 0.5761\n",
      "Epoch 402/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.0000 - acc: 0.5512 - val_loss: -3.6162 - val_acc: 0.5761\n",
      "Epoch 403/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -5.9999 - acc: 0.5512 - val_loss: -3.6188 - val_acc: 0.5761\n",
      "Epoch 404/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.0015 - acc: 0.5512 - val_loss: -3.6188 - val_acc: 0.5761\n",
      "Epoch 405/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.0026 - acc: 0.5497 - val_loss: -3.6156 - val_acc: 0.5761\n",
      "Epoch 406/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.0003 - acc: 0.5497 - val_loss: -3.6184 - val_acc: 0.5761\n",
      "Epoch 407/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.0018 - acc: 0.5497 - val_loss: -3.6179 - val_acc: 0.5761\n",
      "Epoch 408/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.0046 - acc: 0.5481 - val_loss: -3.6172 - val_acc: 0.5761\n",
      "Epoch 409/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644/644 [==============================] - 0s 103us/step - loss: -6.0026 - acc: 0.5481 - val_loss: -3.6174 - val_acc: 0.5761\n",
      "Epoch 410/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.0065 - acc: 0.5481 - val_loss: -3.6170 - val_acc: 0.5761\n",
      "Epoch 411/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.0040 - acc: 0.5497 - val_loss: -3.6163 - val_acc: 0.5761\n",
      "Epoch 412/1000\n",
      "644/644 [==============================] - 0s 112us/step - loss: -6.0072 - acc: 0.5512 - val_loss: -3.6147 - val_acc: 0.5725\n",
      "Epoch 413/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.0094 - acc: 0.5497 - val_loss: -3.6146 - val_acc: 0.5725\n",
      "Epoch 414/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.0074 - acc: 0.5497 - val_loss: -3.6149 - val_acc: 0.5761\n",
      "Epoch 415/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.0078 - acc: 0.5481 - val_loss: -3.6140 - val_acc: 0.5761\n",
      "Epoch 416/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.0109 - acc: 0.5481 - val_loss: -3.6139 - val_acc: 0.5725\n",
      "Epoch 417/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.0085 - acc: 0.5497 - val_loss: -3.6140 - val_acc: 0.5761\n",
      "Epoch 418/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.0102 - acc: 0.5512 - val_loss: -3.6140 - val_acc: 0.5761\n",
      "Epoch 419/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.0141 - acc: 0.5497 - val_loss: -3.6135 - val_acc: 0.5725\n",
      "Epoch 420/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.0144 - acc: 0.5497 - val_loss: -3.6136 - val_acc: 0.5761\n",
      "Epoch 421/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -6.0144 - acc: 0.5481 - val_loss: -3.6130 - val_acc: 0.5761\n",
      "Epoch 422/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.0176 - acc: 0.5497 - val_loss: -3.6132 - val_acc: 0.5761\n",
      "Epoch 423/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.0154 - acc: 0.5497 - val_loss: -3.6128 - val_acc: 0.5761\n",
      "Epoch 424/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.0150 - acc: 0.5497 - val_loss: -3.6127 - val_acc: 0.5761\n",
      "Epoch 425/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.0184 - acc: 0.5481 - val_loss: -3.6116 - val_acc: 0.5761\n",
      "Epoch 426/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -6.0196 - acc: 0.5497 - val_loss: -3.6118 - val_acc: 0.5761\n",
      "Epoch 427/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.0181 - acc: 0.5497 - val_loss: -3.6123 - val_acc: 0.5761\n",
      "Epoch 428/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.0208 - acc: 0.5481 - val_loss: -3.6114 - val_acc: 0.5761\n",
      "Epoch 429/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.0205 - acc: 0.5481 - val_loss: -3.6101 - val_acc: 0.5761\n",
      "Epoch 430/1000\n",
      "644/644 [==============================] - 0s 95us/step - loss: -6.0209 - acc: 0.5497 - val_loss: -3.6098 - val_acc: 0.5725\n",
      "Epoch 431/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.0229 - acc: 0.5481 - val_loss: -3.6097 - val_acc: 0.5761\n",
      "Epoch 432/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.0233 - acc: 0.5497 - val_loss: -3.6100 - val_acc: 0.5761\n",
      "Epoch 433/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.0239 - acc: 0.5497 - val_loss: -3.6089 - val_acc: 0.5761\n",
      "Epoch 434/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -6.0241 - acc: 0.5512 - val_loss: -3.6087 - val_acc: 0.5761\n",
      "Epoch 435/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.0278 - acc: 0.5512 - val_loss: -3.6082 - val_acc: 0.5761\n",
      "Epoch 436/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.0286 - acc: 0.5512 - val_loss: -3.6076 - val_acc: 0.5761\n",
      "Epoch 437/1000\n",
      "644/644 [==============================] - 0s 121us/step - loss: -6.0301 - acc: 0.5512 - val_loss: -3.6129 - val_acc: 0.5761\n",
      "Epoch 438/1000\n",
      "644/644 [==============================] - 0s 122us/step - loss: -6.0297 - acc: 0.5497 - val_loss: -3.6136 - val_acc: 0.5761\n",
      "Epoch 439/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.0297 - acc: 0.5512 - val_loss: -3.6081 - val_acc: 0.5761\n",
      "Epoch 440/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.0290 - acc: 0.5512 - val_loss: -3.6130 - val_acc: 0.5761\n",
      "Epoch 441/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.0313 - acc: 0.5497 - val_loss: -3.6123 - val_acc: 0.5761\n",
      "Epoch 442/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.0321 - acc: 0.5512 - val_loss: -3.6118 - val_acc: 0.5761\n",
      "Epoch 443/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.0334 - acc: 0.5512 - val_loss: -3.6114 - val_acc: 0.5761\n",
      "Epoch 444/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.0312 - acc: 0.5497 - val_loss: -3.6121 - val_acc: 0.5761\n",
      "Epoch 445/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -6.0338 - acc: 0.5528 - val_loss: -3.6117 - val_acc: 0.5761\n",
      "Epoch 446/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.0359 - acc: 0.5512 - val_loss: -3.6113 - val_acc: 0.5761\n",
      "Epoch 447/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.0344 - acc: 0.5512 - val_loss: -3.6111 - val_acc: 0.5761\n",
      "Epoch 448/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -6.0342 - acc: 0.5497 - val_loss: -3.6103 - val_acc: 0.5761\n",
      "Epoch 449/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.0348 - acc: 0.5497 - val_loss: -3.6098 - val_acc: 0.5761\n",
      "Epoch 450/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.0353 - acc: 0.5497 - val_loss: -3.6099 - val_acc: 0.5761\n",
      "Epoch 451/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.0363 - acc: 0.5512 - val_loss: -3.6096 - val_acc: 0.5761\n",
      "Epoch 452/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.0376 - acc: 0.5512 - val_loss: -3.6090 - val_acc: 0.5761\n",
      "Epoch 453/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.0379 - acc: 0.5512 - val_loss: -3.6084 - val_acc: 0.5761\n",
      "Epoch 454/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.0371 - acc: 0.5497 - val_loss: -3.6092 - val_acc: 0.5761\n",
      "Epoch 455/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.0378 - acc: 0.5481 - val_loss: -3.6085 - val_acc: 0.5761\n",
      "Epoch 456/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -6.0393 - acc: 0.5512 - val_loss: -3.6080 - val_acc: 0.5761\n",
      "Epoch 457/1000\n",
      "644/644 [==============================] - 0s 124us/step - loss: -6.0402 - acc: 0.5512 - val_loss: -3.6081 - val_acc: 0.5761\n",
      "Epoch 458/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.0405 - acc: 0.5497 - val_loss: -3.6072 - val_acc: 0.5761\n",
      "Epoch 459/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.0416 - acc: 0.5481 - val_loss: -3.6071 - val_acc: 0.5761\n",
      "Epoch 460/1000\n",
      "644/644 [==============================] - 0s 114us/step - loss: -6.0431 - acc: 0.5497 - val_loss: -3.6066 - val_acc: 0.5761\n",
      "Epoch 461/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -6.0427 - acc: 0.5481 - val_loss: -3.6064 - val_acc: 0.5797\n",
      "Epoch 462/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.0450 - acc: 0.5512 - val_loss: -3.6058 - val_acc: 0.5761\n",
      "Epoch 463/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.0435 - acc: 0.5497 - val_loss: -3.6059 - val_acc: 0.5761\n",
      "Epoch 464/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.0446 - acc: 0.5512 - val_loss: -3.6049 - val_acc: 0.5761\n",
      "Epoch 465/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.0462 - acc: 0.5512 - val_loss: -3.6053 - val_acc: 0.5761\n",
      "Epoch 466/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.0460 - acc: 0.5512 - val_loss: -3.6057 - val_acc: 0.5797\n",
      "Epoch 467/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644/644 [==============================] - 0s 97us/step - loss: -6.0475 - acc: 0.5512 - val_loss: -3.6066 - val_acc: 0.5797\n",
      "Epoch 468/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.0477 - acc: 0.5512 - val_loss: -3.6067 - val_acc: 0.5761\n",
      "Epoch 469/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.0475 - acc: 0.5528 - val_loss: -3.6061 - val_acc: 0.5761\n",
      "Epoch 470/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.0478 - acc: 0.5528 - val_loss: -3.6068 - val_acc: 0.5761\n",
      "Epoch 471/1000\n",
      "644/644 [==============================] - 0s 115us/step - loss: -6.0486 - acc: 0.5512 - val_loss: -3.6055 - val_acc: 0.5797\n",
      "Epoch 472/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.0493 - acc: 0.5512 - val_loss: -3.6059 - val_acc: 0.5761\n",
      "Epoch 473/1000\n",
      "644/644 [==============================] - ETA: 0s - loss: -9.1603 - acc: 0.52 - 0s 99us/step - loss: -6.0499 - acc: 0.5512 - val_loss: -3.6062 - val_acc: 0.5797\n",
      "Epoch 474/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.0513 - acc: 0.5497 - val_loss: -3.6060 - val_acc: 0.5797\n",
      "Epoch 475/1000\n",
      "644/644 [==============================] - 0s 112us/step - loss: -6.0505 - acc: 0.5512 - val_loss: -3.6060 - val_acc: 0.5761\n",
      "Epoch 476/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.0535 - acc: 0.5528 - val_loss: -3.6063 - val_acc: 0.5761\n",
      "Epoch 477/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.0527 - acc: 0.5528 - val_loss: -3.6057 - val_acc: 0.5761\n",
      "Epoch 478/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.0530 - acc: 0.5512 - val_loss: -3.6053 - val_acc: 0.5761\n",
      "Epoch 479/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.0532 - acc: 0.5512 - val_loss: -3.6064 - val_acc: 0.5761\n",
      "Epoch 480/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.0544 - acc: 0.5497 - val_loss: -3.6058 - val_acc: 0.5761\n",
      "Epoch 481/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.0544 - acc: 0.5543 - val_loss: -3.6061 - val_acc: 0.5725\n",
      "Epoch 482/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.0563 - acc: 0.5512 - val_loss: -3.6064 - val_acc: 0.5761\n",
      "Epoch 483/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.0541 - acc: 0.5512 - val_loss: -3.6069 - val_acc: 0.5761\n",
      "Epoch 484/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.0553 - acc: 0.5528 - val_loss: -3.6066 - val_acc: 0.5761\n",
      "Epoch 485/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.0552 - acc: 0.5497 - val_loss: -3.6061 - val_acc: 0.5761\n",
      "Epoch 486/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.0559 - acc: 0.5512 - val_loss: -3.6042 - val_acc: 0.5761\n",
      "Epoch 487/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.0573 - acc: 0.5497 - val_loss: -3.6041 - val_acc: 0.5797\n",
      "Epoch 488/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -6.0562 - acc: 0.5512 - val_loss: -3.6047 - val_acc: 0.5797\n",
      "Epoch 489/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -6.0594 - acc: 0.5497 - val_loss: -3.6048 - val_acc: 0.5797\n",
      "Epoch 490/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.0610 - acc: 0.5512 - val_loss: -3.6044 - val_acc: 0.5797\n",
      "Epoch 491/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.0577 - acc: 0.5512 - val_loss: -3.6049 - val_acc: 0.5797\n",
      "Epoch 492/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.0575 - acc: 0.5528 - val_loss: -3.6044 - val_acc: 0.5797\n",
      "Epoch 493/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.0588 - acc: 0.5497 - val_loss: -3.6046 - val_acc: 0.5797\n",
      "Epoch 494/1000\n",
      "644/644 [==============================] - 0s 94us/step - loss: -6.0596 - acc: 0.5512 - val_loss: -3.6040 - val_acc: 0.5797\n",
      "Epoch 495/1000\n",
      "644/644 [==============================] - 0s 112us/step - loss: -6.0613 - acc: 0.5512 - val_loss: -3.6042 - val_acc: 0.5797\n",
      "Epoch 496/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.0633 - acc: 0.5497 - val_loss: -3.6042 - val_acc: 0.5797\n",
      "Epoch 497/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.0645 - acc: 0.5528 - val_loss: -3.6047 - val_acc: 0.5797\n",
      "Epoch 498/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.0622 - acc: 0.5497 - val_loss: -3.6046 - val_acc: 0.5797\n",
      "Epoch 499/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -6.0636 - acc: 0.5528 - val_loss: -3.6041 - val_acc: 0.5797\n",
      "Epoch 500/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.0655 - acc: 0.5497 - val_loss: -3.6054 - val_acc: 0.5797\n",
      "Epoch 501/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.0641 - acc: 0.5528 - val_loss: -3.6055 - val_acc: 0.5797\n",
      "Epoch 502/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.0637 - acc: 0.5528 - val_loss: -3.6054 - val_acc: 0.5797\n",
      "Epoch 503/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -6.0645 - acc: 0.5528 - val_loss: -3.6062 - val_acc: 0.5797\n",
      "Epoch 504/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.0648 - acc: 0.5543 - val_loss: -3.6041 - val_acc: 0.5797\n",
      "Epoch 505/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.0677 - acc: 0.5528 - val_loss: -3.6025 - val_acc: 0.5797\n",
      "Epoch 506/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.0679 - acc: 0.5543 - val_loss: -3.6023 - val_acc: 0.5797\n",
      "Epoch 507/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.0702 - acc: 0.5528 - val_loss: -3.6026 - val_acc: 0.5797\n",
      "Epoch 508/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -6.0687 - acc: 0.5528 - val_loss: -3.6032 - val_acc: 0.5797\n",
      "Epoch 509/1000\n",
      "644/644 [==============================] - 0s 121us/step - loss: -6.0679 - acc: 0.5528 - val_loss: -3.6035 - val_acc: 0.5797\n",
      "Epoch 510/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.0687 - acc: 0.5528 - val_loss: -3.6032 - val_acc: 0.5797\n",
      "Epoch 511/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.0700 - acc: 0.5528 - val_loss: -3.6034 - val_acc: 0.5797\n",
      "Epoch 512/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.0714 - acc: 0.5528 - val_loss: -3.6038 - val_acc: 0.5797\n",
      "Epoch 513/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.0753 - acc: 0.5528 - val_loss: -3.6034 - val_acc: 0.5797\n",
      "Epoch 514/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.0727 - acc: 0.5528 - val_loss: -3.6034 - val_acc: 0.5797\n",
      "Epoch 515/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.0738 - acc: 0.5528 - val_loss: -3.6030 - val_acc: 0.5797\n",
      "Epoch 516/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.0742 - acc: 0.5543 - val_loss: -3.6041 - val_acc: 0.5797\n",
      "Epoch 517/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.0754 - acc: 0.5528 - val_loss: -3.6036 - val_acc: 0.5797\n",
      "Epoch 518/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.0752 - acc: 0.5528 - val_loss: -3.6041 - val_acc: 0.5797\n",
      "Epoch 519/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.0773 - acc: 0.5528 - val_loss: -3.6040 - val_acc: 0.5797\n",
      "Epoch 520/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.0772 - acc: 0.5528 - val_loss: -3.6043 - val_acc: 0.5797\n",
      "Epoch 521/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.0751 - acc: 0.5528 - val_loss: -3.6039 - val_acc: 0.5797\n",
      "Epoch 522/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -6.0780 - acc: 0.5543 - val_loss: -3.6048 - val_acc: 0.5797\n",
      "Epoch 523/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.0766 - acc: 0.5528 - val_loss: -3.6047 - val_acc: 0.5797\n",
      "Epoch 524/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.0787 - acc: 0.5543 - val_loss: -3.6053 - val_acc: 0.5797\n",
      "Epoch 525/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644/644 [==============================] - 0s 102us/step - loss: -6.0801 - acc: 0.5528 - val_loss: -3.6058 - val_acc: 0.5797\n",
      "Epoch 526/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.0795 - acc: 0.5543 - val_loss: -3.6060 - val_acc: 0.5797\n",
      "Epoch 527/1000\n",
      "644/644 [==============================] - 0s 116us/step - loss: -6.0807 - acc: 0.5528 - val_loss: -3.6067 - val_acc: 0.5797\n",
      "Epoch 528/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -6.0811 - acc: 0.5528 - val_loss: -3.6064 - val_acc: 0.5797\n",
      "Epoch 529/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.0827 - acc: 0.5543 - val_loss: -3.6059 - val_acc: 0.5797\n",
      "Epoch 530/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.0828 - acc: 0.5543 - val_loss: -3.6070 - val_acc: 0.5797\n",
      "Epoch 531/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.0817 - acc: 0.5543 - val_loss: -3.6067 - val_acc: 0.5761\n",
      "Epoch 532/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.0847 - acc: 0.5528 - val_loss: -3.6062 - val_acc: 0.5761\n",
      "Epoch 533/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.0850 - acc: 0.5543 - val_loss: -3.6076 - val_acc: 0.5761\n",
      "Epoch 534/1000\n",
      "644/644 [==============================] - 0s 91us/step - loss: -6.0867 - acc: 0.5528 - val_loss: -3.6071 - val_acc: 0.5761\n",
      "Epoch 535/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.0851 - acc: 0.5543 - val_loss: -3.6069 - val_acc: 0.5761\n",
      "Epoch 536/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.0863 - acc: 0.5559 - val_loss: -3.6074 - val_acc: 0.5761\n",
      "Epoch 537/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.0875 - acc: 0.5559 - val_loss: -3.6082 - val_acc: 0.5761\n",
      "Epoch 538/1000\n",
      "644/644 [==============================] - 0s 121us/step - loss: -6.0874 - acc: 0.5559 - val_loss: -3.6089 - val_acc: 0.5761\n",
      "Epoch 539/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.0865 - acc: 0.5559 - val_loss: -3.6086 - val_acc: 0.5761\n",
      "Epoch 540/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.0870 - acc: 0.5575 - val_loss: -3.6083 - val_acc: 0.5761\n",
      "Epoch 541/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.0904 - acc: 0.5575 - val_loss: -3.6090 - val_acc: 0.5761\n",
      "Epoch 542/1000\n",
      "644/644 [==============================] - 0s 124us/step - loss: -6.0922 - acc: 0.5543 - val_loss: -3.6088 - val_acc: 0.5761\n",
      "Epoch 543/1000\n",
      "644/644 [==============================] - 0s 129us/step - loss: -6.0903 - acc: 0.5559 - val_loss: -3.6088 - val_acc: 0.5761\n",
      "Epoch 544/1000\n",
      "644/644 [==============================] - 0s 129us/step - loss: -6.0888 - acc: 0.5575 - val_loss: -3.6082 - val_acc: 0.5761\n",
      "Epoch 545/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -6.0919 - acc: 0.5575 - val_loss: -3.6094 - val_acc: 0.5761\n",
      "Epoch 546/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.0911 - acc: 0.5559 - val_loss: -3.6092 - val_acc: 0.5761\n",
      "Epoch 547/1000\n",
      "644/644 [==============================] - 0s 120us/step - loss: -6.0927 - acc: 0.5559 - val_loss: -3.6086 - val_acc: 0.5761\n",
      "Epoch 548/1000\n",
      "644/644 [==============================] - 0s 116us/step - loss: -6.0937 - acc: 0.5575 - val_loss: -3.6091 - val_acc: 0.5761\n",
      "Epoch 549/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.0959 - acc: 0.5559 - val_loss: -3.6089 - val_acc: 0.5761\n",
      "Epoch 550/1000\n",
      "644/644 [==============================] - 0s 114us/step - loss: -6.0962 - acc: 0.5590 - val_loss: -3.6089 - val_acc: 0.5761\n",
      "Epoch 551/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.0939 - acc: 0.5606 - val_loss: -3.6099 - val_acc: 0.5761\n",
      "Epoch 552/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -6.0954 - acc: 0.5575 - val_loss: -3.6083 - val_acc: 0.5761\n",
      "Epoch 553/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.0986 - acc: 0.5590 - val_loss: -3.6089 - val_acc: 0.5761\n",
      "Epoch 554/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.0963 - acc: 0.5575 - val_loss: -3.6091 - val_acc: 0.5761\n",
      "Epoch 555/1000\n",
      "644/644 [==============================] - 0s 93us/step - loss: -6.0957 - acc: 0.5559 - val_loss: -3.6092 - val_acc: 0.5761\n",
      "Epoch 556/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.0998 - acc: 0.5559 - val_loss: -3.6100 - val_acc: 0.5761\n",
      "Epoch 557/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1003 - acc: 0.5575 - val_loss: -3.6079 - val_acc: 0.5761\n",
      "Epoch 558/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.0974 - acc: 0.5590 - val_loss: -3.6087 - val_acc: 0.5761\n",
      "Epoch 559/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -6.0984 - acc: 0.5575 - val_loss: -3.6080 - val_acc: 0.5761\n",
      "Epoch 560/1000\n",
      "644/644 [==============================] - 0s 90us/step - loss: -6.1013 - acc: 0.5575 - val_loss: -3.6074 - val_acc: 0.5761\n",
      "Epoch 561/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -6.1029 - acc: 0.5575 - val_loss: -3.6087 - val_acc: 0.5761\n",
      "Epoch 562/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.1033 - acc: 0.5575 - val_loss: -3.6079 - val_acc: 0.5761\n",
      "Epoch 563/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1036 - acc: 0.5575 - val_loss: -3.6084 - val_acc: 0.5761\n",
      "Epoch 564/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.1046 - acc: 0.5575 - val_loss: -3.6078 - val_acc: 0.5761\n",
      "Epoch 565/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1054 - acc: 0.5575 - val_loss: -3.6079 - val_acc: 0.5761\n",
      "Epoch 566/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.1058 - acc: 0.5575 - val_loss: -3.6078 - val_acc: 0.5761\n",
      "Epoch 567/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1048 - acc: 0.5575 - val_loss: -3.6072 - val_acc: 0.5761\n",
      "Epoch 568/1000\n",
      "644/644 [==============================] - 0s 95us/step - loss: -6.1065 - acc: 0.5590 - val_loss: -3.6076 - val_acc: 0.5761\n",
      "Epoch 569/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.1062 - acc: 0.5575 - val_loss: -3.6070 - val_acc: 0.5761\n",
      "Epoch 570/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1071 - acc: 0.5575 - val_loss: -3.6082 - val_acc: 0.5761\n",
      "Epoch 571/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.1063 - acc: 0.5575 - val_loss: -3.6083 - val_acc: 0.5761\n",
      "Epoch 572/1000\n",
      "644/644 [==============================] - 0s 118us/step - loss: -6.1086 - acc: 0.5575 - val_loss: -3.6080 - val_acc: 0.5761\n",
      "Epoch 573/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -6.1086 - acc: 0.5575 - val_loss: -3.6074 - val_acc: 0.5761\n",
      "Epoch 574/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.1076 - acc: 0.5575 - val_loss: -3.6086 - val_acc: 0.5761\n",
      "Epoch 575/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1089 - acc: 0.5575 - val_loss: -3.6079 - val_acc: 0.5761\n",
      "Epoch 576/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1082 - acc: 0.5575 - val_loss: -3.6086 - val_acc: 0.5761\n",
      "Epoch 577/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.1106 - acc: 0.5575 - val_loss: -3.6086 - val_acc: 0.5761\n",
      "Epoch 578/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.1079 - acc: 0.5575 - val_loss: -3.6082 - val_acc: 0.5761\n",
      "Epoch 579/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.1095 - acc: 0.5590 - val_loss: -3.6077 - val_acc: 0.5761\n",
      "Epoch 580/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1107 - acc: 0.5575 - val_loss: -3.6070 - val_acc: 0.5797\n",
      "Epoch 581/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.1107 - acc: 0.5575 - val_loss: -3.6066 - val_acc: 0.5797\n",
      "Epoch 582/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.1099 - acc: 0.5575 - val_loss: -3.6062 - val_acc: 0.5797\n",
      "Epoch 583/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1119 - acc: 0.5606 - val_loss: -3.6063 - val_acc: 0.5797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 584/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1108 - acc: 0.5575 - val_loss: -3.6076 - val_acc: 0.5797\n",
      "Epoch 585/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -6.1128 - acc: 0.5606 - val_loss: -3.6069 - val_acc: 0.5797\n",
      "Epoch 586/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -6.1125 - acc: 0.5575 - val_loss: -3.6070 - val_acc: 0.5797\n",
      "Epoch 587/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.1123 - acc: 0.5575 - val_loss: -3.6070 - val_acc: 0.5797\n",
      "Epoch 588/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -6.1143 - acc: 0.5575 - val_loss: -3.6081 - val_acc: 0.5797\n",
      "Epoch 589/1000\n",
      "644/644 [==============================] - 0s 113us/step - loss: -6.1136 - acc: 0.5575 - val_loss: -3.6078 - val_acc: 0.5797\n",
      "Epoch 590/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.1136 - acc: 0.5621 - val_loss: -3.6076 - val_acc: 0.5797\n",
      "Epoch 591/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.1160 - acc: 0.5606 - val_loss: -3.6068 - val_acc: 0.5797\n",
      "Epoch 592/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -6.1144 - acc: 0.5606 - val_loss: -3.6064 - val_acc: 0.5797\n",
      "Epoch 593/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.1147 - acc: 0.5575 - val_loss: -3.6067 - val_acc: 0.5797\n",
      "Epoch 594/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1154 - acc: 0.5606 - val_loss: -3.6073 - val_acc: 0.5797\n",
      "Epoch 595/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.1161 - acc: 0.5590 - val_loss: -3.6082 - val_acc: 0.5797\n",
      "Epoch 596/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.1149 - acc: 0.5621 - val_loss: -3.6082 - val_acc: 0.5797\n",
      "Epoch 597/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.1165 - acc: 0.5575 - val_loss: -3.6081 - val_acc: 0.5797\n",
      "Epoch 598/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1171 - acc: 0.5621 - val_loss: -3.6074 - val_acc: 0.5797\n",
      "Epoch 599/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -6.1171 - acc: 0.5575 - val_loss: -3.6078 - val_acc: 0.5797\n",
      "Epoch 600/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.1176 - acc: 0.5606 - val_loss: -3.6075 - val_acc: 0.5797\n",
      "Epoch 601/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.1174 - acc: 0.5606 - val_loss: -3.6089 - val_acc: 0.5797\n",
      "Epoch 602/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1182 - acc: 0.5606 - val_loss: -3.6082 - val_acc: 0.5797\n",
      "Epoch 603/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1174 - acc: 0.5590 - val_loss: -3.6088 - val_acc: 0.5797\n",
      "Epoch 604/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1188 - acc: 0.5606 - val_loss: -3.6088 - val_acc: 0.5797\n",
      "Epoch 605/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1191 - acc: 0.5606 - val_loss: -3.6091 - val_acc: 0.5797\n",
      "Epoch 606/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1209 - acc: 0.5621 - val_loss: -3.6083 - val_acc: 0.5797\n",
      "Epoch 607/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1205 - acc: 0.5606 - val_loss: -3.6089 - val_acc: 0.5797\n",
      "Epoch 608/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.1208 - acc: 0.5606 - val_loss: -3.6090 - val_acc: 0.5797\n",
      "Epoch 609/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1213 - acc: 0.5606 - val_loss: -3.6084 - val_acc: 0.5797\n",
      "Epoch 610/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1209 - acc: 0.5621 - val_loss: -3.6088 - val_acc: 0.5797\n",
      "Epoch 611/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1209 - acc: 0.5606 - val_loss: -3.6096 - val_acc: 0.5797\n",
      "Epoch 612/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1230 - acc: 0.5606 - val_loss: -3.6091 - val_acc: 0.5797\n",
      "Epoch 613/1000\n",
      "644/644 [==============================] - 0s 112us/step - loss: -6.1217 - acc: 0.5606 - val_loss: -3.6090 - val_acc: 0.5797\n",
      "Epoch 614/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.1212 - acc: 0.5606 - val_loss: -3.6094 - val_acc: 0.5797\n",
      "Epoch 615/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1223 - acc: 0.5606 - val_loss: -3.6099 - val_acc: 0.5797\n",
      "Epoch 616/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.1234 - acc: 0.5606 - val_loss: -3.6103 - val_acc: 0.5797\n",
      "Epoch 617/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1238 - acc: 0.5606 - val_loss: -3.6096 - val_acc: 0.5797\n",
      "Epoch 618/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.1232 - acc: 0.5621 - val_loss: -3.6099 - val_acc: 0.5797\n",
      "Epoch 619/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.1253 - acc: 0.5606 - val_loss: -3.6102 - val_acc: 0.5797\n",
      "Epoch 620/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1244 - acc: 0.5606 - val_loss: -3.6108 - val_acc: 0.5797\n",
      "Epoch 621/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -6.1252 - acc: 0.5606 - val_loss: -3.6103 - val_acc: 0.5797\n",
      "Epoch 622/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1257 - acc: 0.5606 - val_loss: -3.6099 - val_acc: 0.5797\n",
      "Epoch 623/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1257 - acc: 0.5606 - val_loss: -3.6097 - val_acc: 0.5797\n",
      "Epoch 624/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.1256 - acc: 0.5621 - val_loss: -3.6101 - val_acc: 0.5797\n",
      "Epoch 625/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1264 - acc: 0.5606 - val_loss: -3.6107 - val_acc: 0.5797\n",
      "Epoch 626/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.1279 - acc: 0.5606 - val_loss: -3.6103 - val_acc: 0.5797\n",
      "Epoch 627/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1273 - acc: 0.5606 - val_loss: -3.6099 - val_acc: 0.5797\n",
      "Epoch 628/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.1277 - acc: 0.5621 - val_loss: -3.6101 - val_acc: 0.5797\n",
      "Epoch 629/1000\n",
      "644/644 [==============================] - 0s 95us/step - loss: -6.1272 - acc: 0.5606 - val_loss: -3.6100 - val_acc: 0.5797\n",
      "Epoch 630/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.1281 - acc: 0.5621 - val_loss: -3.6091 - val_acc: 0.5797\n",
      "Epoch 631/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.1281 - acc: 0.5606 - val_loss: -3.6105 - val_acc: 0.5797\n",
      "Epoch 632/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1281 - acc: 0.5637 - val_loss: -3.6103 - val_acc: 0.5797\n",
      "Epoch 633/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.1274 - acc: 0.5637 - val_loss: -3.6092 - val_acc: 0.5797\n",
      "Epoch 634/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1289 - acc: 0.5621 - val_loss: -3.6090 - val_acc: 0.5797\n",
      "Epoch 635/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1289 - acc: 0.5621 - val_loss: -3.6063 - val_acc: 0.5797\n",
      "Epoch 636/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1293 - acc: 0.5606 - val_loss: -3.6098 - val_acc: 0.5797\n",
      "Epoch 637/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.1300 - acc: 0.5621 - val_loss: -3.6096 - val_acc: 0.5797\n",
      "Epoch 638/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1301 - acc: 0.5652 - val_loss: -3.6105 - val_acc: 0.5797\n",
      "Epoch 639/1000\n",
      "644/644 [==============================] - 0s 94us/step - loss: -6.1308 - acc: 0.5621 - val_loss: -3.6105 - val_acc: 0.5797\n",
      "Epoch 640/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -6.1318 - acc: 0.5637 - val_loss: -3.6082 - val_acc: 0.5797\n",
      "Epoch 641/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1324 - acc: 0.5637 - val_loss: -3.6108 - val_acc: 0.5797\n",
      "Epoch 642/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644/644 [==============================] - 0s 103us/step - loss: -6.1319 - acc: 0.5652 - val_loss: -3.6086 - val_acc: 0.5797\n",
      "Epoch 643/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.1336 - acc: 0.5668 - val_loss: -3.6109 - val_acc: 0.5797\n",
      "Epoch 644/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1338 - acc: 0.5668 - val_loss: -3.6103 - val_acc: 0.5797\n",
      "Epoch 645/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.1351 - acc: 0.5668 - val_loss: -3.6106 - val_acc: 0.5797\n",
      "Epoch 646/1000\n",
      "644/644 [==============================] - 0s 122us/step - loss: -6.1349 - acc: 0.5668 - val_loss: -3.6099 - val_acc: 0.5797\n",
      "Epoch 647/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1336 - acc: 0.5668 - val_loss: -3.6074 - val_acc: 0.5797\n",
      "Epoch 648/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1354 - acc: 0.5683 - val_loss: -3.6083 - val_acc: 0.5797\n",
      "Epoch 649/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1361 - acc: 0.5668 - val_loss: -3.6087 - val_acc: 0.5797\n",
      "Epoch 650/1000\n",
      "644/644 [==============================] - 0s 112us/step - loss: -6.1368 - acc: 0.5668 - val_loss: -3.6082 - val_acc: 0.5797\n",
      "Epoch 651/1000\n",
      "644/644 [==============================] - 0s 117us/step - loss: -6.1366 - acc: 0.5683 - val_loss: -3.6091 - val_acc: 0.5797\n",
      "Epoch 652/1000\n",
      "644/644 [==============================] - 0s 117us/step - loss: -6.1365 - acc: 0.5683 - val_loss: -3.6097 - val_acc: 0.5797\n",
      "Epoch 653/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -6.1381 - acc: 0.5683 - val_loss: -3.6098 - val_acc: 0.5797\n",
      "Epoch 654/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1370 - acc: 0.5668 - val_loss: -3.6101 - val_acc: 0.5797\n",
      "Epoch 655/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1382 - acc: 0.5683 - val_loss: -3.6099 - val_acc: 0.5797\n",
      "Epoch 656/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.1388 - acc: 0.5683 - val_loss: -3.6105 - val_acc: 0.5797\n",
      "Epoch 657/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1374 - acc: 0.5652 - val_loss: -3.6108 - val_acc: 0.5797\n",
      "Epoch 658/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -6.1393 - acc: 0.5683 - val_loss: -3.6135 - val_acc: 0.5797\n",
      "Epoch 659/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -6.1402 - acc: 0.5683 - val_loss: -3.6110 - val_acc: 0.5797\n",
      "Epoch 660/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1411 - acc: 0.5683 - val_loss: -3.6130 - val_acc: 0.5797\n",
      "Epoch 661/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.1406 - acc: 0.5668 - val_loss: -3.6133 - val_acc: 0.5797\n",
      "Epoch 662/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.1404 - acc: 0.5668 - val_loss: -3.6111 - val_acc: 0.5797\n",
      "Epoch 663/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.1423 - acc: 0.5683 - val_loss: -3.6136 - val_acc: 0.5797\n",
      "Epoch 664/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1415 - acc: 0.5683 - val_loss: -3.6136 - val_acc: 0.5797\n",
      "Epoch 665/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.1424 - acc: 0.5683 - val_loss: -3.6110 - val_acc: 0.5797\n",
      "Epoch 666/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -6.1427 - acc: 0.5683 - val_loss: -3.6124 - val_acc: 0.5797\n",
      "Epoch 667/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.1432 - acc: 0.5683 - val_loss: -3.6119 - val_acc: 0.5797\n",
      "Epoch 668/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.1432 - acc: 0.5683 - val_loss: -3.6142 - val_acc: 0.5797\n",
      "Epoch 669/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1441 - acc: 0.5683 - val_loss: -3.6155 - val_acc: 0.5797\n",
      "Epoch 670/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -6.1445 - acc: 0.5683 - val_loss: -3.6159 - val_acc: 0.5797\n",
      "Epoch 671/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.1462 - acc: 0.5668 - val_loss: -3.6158 - val_acc: 0.5797\n",
      "Epoch 672/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.1461 - acc: 0.5668 - val_loss: -3.6153 - val_acc: 0.5797\n",
      "Epoch 673/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.1472 - acc: 0.5668 - val_loss: -3.6155 - val_acc: 0.5797\n",
      "Epoch 674/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1462 - acc: 0.5668 - val_loss: -3.6153 - val_acc: 0.5797\n",
      "Epoch 675/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1478 - acc: 0.5668 - val_loss: -3.6160 - val_acc: 0.5797\n",
      "Epoch 676/1000\n",
      "644/644 [==============================] - 0s 95us/step - loss: -6.1480 - acc: 0.5668 - val_loss: -3.6154 - val_acc: 0.5797\n",
      "Epoch 677/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1469 - acc: 0.5668 - val_loss: -3.6172 - val_acc: 0.5797\n",
      "Epoch 678/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -6.1473 - acc: 0.5683 - val_loss: -3.6173 - val_acc: 0.5797\n",
      "Epoch 679/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1498 - acc: 0.5668 - val_loss: -3.6173 - val_acc: 0.5797\n",
      "Epoch 680/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -6.1490 - acc: 0.5683 - val_loss: -3.6172 - val_acc: 0.5797\n",
      "Epoch 681/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1490 - acc: 0.5668 - val_loss: -3.6173 - val_acc: 0.5797\n",
      "Epoch 682/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1496 - acc: 0.5637 - val_loss: -3.6168 - val_acc: 0.5797\n",
      "Epoch 683/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.1508 - acc: 0.5668 - val_loss: -3.6181 - val_acc: 0.5797\n",
      "Epoch 684/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.1507 - acc: 0.5668 - val_loss: -3.6189 - val_acc: 0.5797\n",
      "Epoch 685/1000\n",
      "644/644 [==============================] - 0s 95us/step - loss: -6.1513 - acc: 0.5652 - val_loss: -3.6188 - val_acc: 0.5797\n",
      "Epoch 686/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1511 - acc: 0.5668 - val_loss: -3.6195 - val_acc: 0.5797\n",
      "Epoch 687/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.1518 - acc: 0.5668 - val_loss: -3.6202 - val_acc: 0.5797\n",
      "Epoch 688/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1519 - acc: 0.5652 - val_loss: -3.6197 - val_acc: 0.5797\n",
      "Epoch 689/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1537 - acc: 0.5637 - val_loss: -3.6192 - val_acc: 0.5797\n",
      "Epoch 690/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1538 - acc: 0.5652 - val_loss: -3.6193 - val_acc: 0.5797\n",
      "Epoch 691/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1527 - acc: 0.5668 - val_loss: -3.6192 - val_acc: 0.5797\n",
      "Epoch 692/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1536 - acc: 0.5668 - val_loss: -3.6204 - val_acc: 0.5797\n",
      "Epoch 693/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1550 - acc: 0.5652 - val_loss: -3.6199 - val_acc: 0.5797\n",
      "Epoch 694/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.1542 - acc: 0.5652 - val_loss: -3.6210 - val_acc: 0.5797\n",
      "Epoch 695/1000\n",
      "644/644 [==============================] - 0s 95us/step - loss: -6.1547 - acc: 0.5668 - val_loss: -3.6208 - val_acc: 0.5797\n",
      "Epoch 696/1000\n",
      "644/644 [==============================] - 0s 112us/step - loss: -6.1554 - acc: 0.5652 - val_loss: -3.6215 - val_acc: 0.5797\n",
      "Epoch 697/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1567 - acc: 0.5652 - val_loss: -3.6206 - val_acc: 0.5797\n",
      "Epoch 698/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.1567 - acc: 0.5652 - val_loss: -3.6205 - val_acc: 0.5797\n",
      "Epoch 699/1000\n",
      "644/644 [==============================] - 0s 112us/step - loss: -6.1569 - acc: 0.5668 - val_loss: -3.6206 - val_acc: 0.5797\n",
      "Epoch 700/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1582 - acc: 0.5668 - val_loss: -3.6212 - val_acc: 0.5797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 701/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.1574 - acc: 0.5652 - val_loss: -3.6216 - val_acc: 0.5797\n",
      "Epoch 702/1000\n",
      "644/644 [==============================] - 0s 114us/step - loss: -6.1582 - acc: 0.5652 - val_loss: -3.6216 - val_acc: 0.5797\n",
      "Epoch 703/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1588 - acc: 0.5668 - val_loss: -3.6219 - val_acc: 0.5797\n",
      "Epoch 704/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1592 - acc: 0.5668 - val_loss: -3.6224 - val_acc: 0.5797\n",
      "Epoch 705/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.1589 - acc: 0.5652 - val_loss: -3.6212 - val_acc: 0.5797\n",
      "Epoch 706/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1596 - acc: 0.5668 - val_loss: -3.6215 - val_acc: 0.5797\n",
      "Epoch 707/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.1588 - acc: 0.5683 - val_loss: -3.6233 - val_acc: 0.5797\n",
      "Epoch 708/1000\n",
      "644/644 [==============================] - 0s 91us/step - loss: -6.1604 - acc: 0.5652 - val_loss: -3.6228 - val_acc: 0.5797\n",
      "Epoch 709/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1616 - acc: 0.5668 - val_loss: -3.6223 - val_acc: 0.5797\n",
      "Epoch 710/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1610 - acc: 0.5668 - val_loss: -3.6226 - val_acc: 0.5797\n",
      "Epoch 711/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1623 - acc: 0.5652 - val_loss: -3.6226 - val_acc: 0.5797\n",
      "Epoch 712/1000\n",
      "644/644 [==============================] - 0s 114us/step - loss: -6.1627 - acc: 0.5668 - val_loss: -3.6235 - val_acc: 0.5797\n",
      "Epoch 713/1000\n",
      "644/644 [==============================] - 0s 116us/step - loss: -6.1627 - acc: 0.5668 - val_loss: -3.6236 - val_acc: 0.5797\n",
      "Epoch 714/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1635 - acc: 0.5668 - val_loss: -3.6242 - val_acc: 0.5797\n",
      "Epoch 715/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.1637 - acc: 0.5652 - val_loss: -3.6239 - val_acc: 0.5797\n",
      "Epoch 716/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1638 - acc: 0.5668 - val_loss: -3.6235 - val_acc: 0.5797\n",
      "Epoch 717/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1654 - acc: 0.5668 - val_loss: -3.6235 - val_acc: 0.5797\n",
      "Epoch 718/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1638 - acc: 0.5668 - val_loss: -3.6239 - val_acc: 0.5833\n",
      "Epoch 719/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1654 - acc: 0.5668 - val_loss: -3.6239 - val_acc: 0.5833\n",
      "Epoch 720/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1659 - acc: 0.5668 - val_loss: -3.6254 - val_acc: 0.5833\n",
      "Epoch 721/1000\n",
      "644/644 [==============================] - 0s 93us/step - loss: -6.1653 - acc: 0.5668 - val_loss: -3.6254 - val_acc: 0.5833\n",
      "Epoch 722/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.1669 - acc: 0.5668 - val_loss: -3.6251 - val_acc: 0.5833\n",
      "Epoch 723/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1680 - acc: 0.5668 - val_loss: -3.6257 - val_acc: 0.5833\n",
      "Epoch 724/1000\n",
      "644/644 [==============================] - 0s 94us/step - loss: -6.1678 - acc: 0.5668 - val_loss: -3.6262 - val_acc: 0.5833\n",
      "Epoch 725/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -6.1697 - acc: 0.5683 - val_loss: -3.6265 - val_acc: 0.5833\n",
      "Epoch 726/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1682 - acc: 0.5668 - val_loss: -3.6266 - val_acc: 0.5833\n",
      "Epoch 727/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1679 - acc: 0.5683 - val_loss: -3.6265 - val_acc: 0.5833\n",
      "Epoch 728/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.1676 - acc: 0.5683 - val_loss: -3.6266 - val_acc: 0.5833\n",
      "Epoch 729/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1705 - acc: 0.5699 - val_loss: -3.6272 - val_acc: 0.5833\n",
      "Epoch 730/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.1701 - acc: 0.5668 - val_loss: -3.6272 - val_acc: 0.5833\n",
      "Epoch 731/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1702 - acc: 0.5699 - val_loss: -3.6268 - val_acc: 0.5833\n",
      "Epoch 732/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.1713 - acc: 0.5699 - val_loss: -3.6263 - val_acc: 0.5833\n",
      "Epoch 733/1000\n",
      "644/644 [==============================] - 0s 93us/step - loss: -6.1697 - acc: 0.5699 - val_loss: -3.6265 - val_acc: 0.5833\n",
      "Epoch 734/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1729 - acc: 0.5699 - val_loss: -3.6270 - val_acc: 0.5833\n",
      "Epoch 735/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.1716 - acc: 0.5652 - val_loss: -3.6283 - val_acc: 0.5833\n",
      "Epoch 736/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.1712 - acc: 0.5668 - val_loss: -3.6283 - val_acc: 0.5833\n",
      "Epoch 737/1000\n",
      "644/644 [==============================] - 0s 121us/step - loss: -6.1726 - acc: 0.5683 - val_loss: -3.6278 - val_acc: 0.5833\n",
      "Epoch 738/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.1734 - acc: 0.5683 - val_loss: -3.6275 - val_acc: 0.5833\n",
      "Epoch 739/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.1748 - acc: 0.5683 - val_loss: -3.6277 - val_acc: 0.5833\n",
      "Epoch 740/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1741 - acc: 0.5683 - val_loss: -3.6281 - val_acc: 0.5833\n",
      "Epoch 741/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.1743 - acc: 0.5683 - val_loss: -3.6272 - val_acc: 0.5833\n",
      "Epoch 742/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -6.1744 - acc: 0.5683 - val_loss: -3.6271 - val_acc: 0.5833\n",
      "Epoch 743/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1748 - acc: 0.5683 - val_loss: -3.6276 - val_acc: 0.5870\n",
      "Epoch 744/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1772 - acc: 0.5683 - val_loss: -3.6271 - val_acc: 0.5833\n",
      "Epoch 745/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1752 - acc: 0.5668 - val_loss: -3.6270 - val_acc: 0.5833\n",
      "Epoch 746/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1769 - acc: 0.5668 - val_loss: -3.6287 - val_acc: 0.5833\n",
      "Epoch 747/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.1784 - acc: 0.5668 - val_loss: -3.6273 - val_acc: 0.5833\n",
      "Epoch 748/1000\n",
      "644/644 [==============================] - ETA: 0s - loss: -7.1767 - acc: 0.60 - 0s 93us/step - loss: -6.1767 - acc: 0.5668 - val_loss: -3.6276 - val_acc: 0.5833\n",
      "Epoch 749/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1777 - acc: 0.5668 - val_loss: -3.6280 - val_acc: 0.5833\n",
      "Epoch 750/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.1787 - acc: 0.5683 - val_loss: -3.6283 - val_acc: 0.5833\n",
      "Epoch 751/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.1792 - acc: 0.5683 - val_loss: -3.6272 - val_acc: 0.5833\n",
      "Epoch 752/1000\n",
      "644/644 [==============================] - 0s 93us/step - loss: -6.1790 - acc: 0.5668 - val_loss: -3.6275 - val_acc: 0.5833\n",
      "Epoch 753/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1793 - acc: 0.5683 - val_loss: -3.6289 - val_acc: 0.5833\n",
      "Epoch 754/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.1793 - acc: 0.5668 - val_loss: -3.6287 - val_acc: 0.5833\n",
      "Epoch 755/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1801 - acc: 0.5683 - val_loss: -3.6291 - val_acc: 0.5870\n",
      "Epoch 756/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1803 - acc: 0.5668 - val_loss: -3.6286 - val_acc: 0.5870\n",
      "Epoch 757/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1807 - acc: 0.5683 - val_loss: -3.6287 - val_acc: 0.5870\n",
      "Epoch 758/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.1809 - acc: 0.5683 - val_loss: -3.6286 - val_acc: 0.5870\n",
      "Epoch 759/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644/644 [==============================] - 0s 99us/step - loss: -6.1821 - acc: 0.5683 - val_loss: -3.6290 - val_acc: 0.5870\n",
      "Epoch 760/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1823 - acc: 0.5683 - val_loss: -3.6292 - val_acc: 0.5870\n",
      "Epoch 761/1000\n",
      "644/644 [==============================] - 0s 121us/step - loss: -6.1828 - acc: 0.5668 - val_loss: -3.6290 - val_acc: 0.5870\n",
      "Epoch 762/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.1829 - acc: 0.5683 - val_loss: -3.6290 - val_acc: 0.5870\n",
      "Epoch 763/1000\n",
      "644/644 [==============================] - 0s 94us/step - loss: -6.1836 - acc: 0.5699 - val_loss: -3.6291 - val_acc: 0.5870\n",
      "Epoch 764/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.1831 - acc: 0.5683 - val_loss: -3.6295 - val_acc: 0.5870\n",
      "Epoch 765/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1846 - acc: 0.5683 - val_loss: -3.6297 - val_acc: 0.5870\n",
      "Epoch 766/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1848 - acc: 0.5668 - val_loss: -3.6296 - val_acc: 0.5833\n",
      "Epoch 767/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.1849 - acc: 0.5683 - val_loss: -3.6302 - val_acc: 0.5870\n",
      "Epoch 768/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.1852 - acc: 0.5668 - val_loss: -3.6292 - val_acc: 0.5870\n",
      "Epoch 769/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.1865 - acc: 0.5668 - val_loss: -3.6304 - val_acc: 0.5870\n",
      "Epoch 770/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.1854 - acc: 0.5652 - val_loss: -3.6296 - val_acc: 0.5906\n",
      "Epoch 771/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.1854 - acc: 0.5668 - val_loss: -3.6296 - val_acc: 0.5870\n",
      "Epoch 772/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1870 - acc: 0.5699 - val_loss: -3.6299 - val_acc: 0.5906\n",
      "Epoch 773/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.1870 - acc: 0.5683 - val_loss: -3.6310 - val_acc: 0.5906\n",
      "Epoch 774/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.1884 - acc: 0.5683 - val_loss: -3.6303 - val_acc: 0.5906\n",
      "Epoch 775/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1885 - acc: 0.5683 - val_loss: -3.6293 - val_acc: 0.5906\n",
      "Epoch 776/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1896 - acc: 0.5668 - val_loss: -3.6299 - val_acc: 0.5906\n",
      "Epoch 777/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -6.1903 - acc: 0.5699 - val_loss: -3.6316 - val_acc: 0.5906\n",
      "Epoch 778/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1885 - acc: 0.5699 - val_loss: -3.6301 - val_acc: 0.5906\n",
      "Epoch 779/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.1912 - acc: 0.5699 - val_loss: -3.6298 - val_acc: 0.5906\n",
      "Epoch 780/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.1908 - acc: 0.5683 - val_loss: -3.6298 - val_acc: 0.5906\n",
      "Epoch 781/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1922 - acc: 0.5699 - val_loss: -3.6286 - val_acc: 0.5906\n",
      "Epoch 782/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1924 - acc: 0.5683 - val_loss: -3.6297 - val_acc: 0.5906\n",
      "Epoch 783/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.1924 - acc: 0.5699 - val_loss: -3.6299 - val_acc: 0.5906\n",
      "Epoch 784/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.1926 - acc: 0.5699 - val_loss: -3.6303 - val_acc: 0.5906\n",
      "Epoch 785/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1943 - acc: 0.5683 - val_loss: -3.6290 - val_acc: 0.5906\n",
      "Epoch 786/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1936 - acc: 0.5699 - val_loss: -3.6301 - val_acc: 0.5906\n",
      "Epoch 787/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1924 - acc: 0.5699 - val_loss: -3.6311 - val_acc: 0.5906\n",
      "Epoch 788/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1932 - acc: 0.5683 - val_loss: -3.6286 - val_acc: 0.5906\n",
      "Epoch 789/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1944 - acc: 0.5699 - val_loss: -3.6297 - val_acc: 0.5906\n",
      "Epoch 790/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1957 - acc: 0.5699 - val_loss: -3.6301 - val_acc: 0.5906\n",
      "Epoch 791/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1940 - acc: 0.5699 - val_loss: -3.6298 - val_acc: 0.5906\n",
      "Epoch 792/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.1971 - acc: 0.5699 - val_loss: -3.6296 - val_acc: 0.5906\n",
      "Epoch 793/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1971 - acc: 0.5699 - val_loss: -3.6296 - val_acc: 0.5906\n",
      "Epoch 794/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.1981 - acc: 0.5714 - val_loss: -3.6308 - val_acc: 0.5870\n",
      "Epoch 795/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1964 - acc: 0.5714 - val_loss: -3.6303 - val_acc: 0.5906\n",
      "Epoch 796/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1972 - acc: 0.5699 - val_loss: -3.6303 - val_acc: 0.5870\n",
      "Epoch 797/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.1990 - acc: 0.5714 - val_loss: -3.6298 - val_acc: 0.5870\n",
      "Epoch 798/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.1966 - acc: 0.5699 - val_loss: -3.6296 - val_acc: 0.5906\n",
      "Epoch 799/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.1986 - acc: 0.5699 - val_loss: -3.6301 - val_acc: 0.5906\n",
      "Epoch 800/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -6.2002 - acc: 0.5699 - val_loss: -3.6300 - val_acc: 0.5906\n",
      "Epoch 801/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.2009 - acc: 0.5714 - val_loss: -3.6299 - val_acc: 0.5906\n",
      "Epoch 802/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.1988 - acc: 0.5730 - val_loss: -3.6292 - val_acc: 0.5870\n",
      "Epoch 803/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.2010 - acc: 0.5730 - val_loss: -3.6298 - val_acc: 0.5870\n",
      "Epoch 804/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2019 - acc: 0.5714 - val_loss: -3.6302 - val_acc: 0.5906\n",
      "Epoch 805/1000\n",
      "644/644 [==============================] - 0s 93us/step - loss: -6.2006 - acc: 0.5730 - val_loss: -3.6314 - val_acc: 0.5906\n",
      "Epoch 806/1000\n",
      "644/644 [==============================] - 0s 95us/step - loss: -6.2017 - acc: 0.5730 - val_loss: -3.6310 - val_acc: 0.5906\n",
      "Epoch 807/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.2025 - acc: 0.5714 - val_loss: -3.6302 - val_acc: 0.5906\n",
      "Epoch 808/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.2024 - acc: 0.5730 - val_loss: -3.6316 - val_acc: 0.5870\n",
      "Epoch 809/1000\n",
      "644/644 [==============================] - 0s 94us/step - loss: -6.2016 - acc: 0.5730 - val_loss: -3.6313 - val_acc: 0.5870\n",
      "Epoch 810/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.2049 - acc: 0.5714 - val_loss: -3.6307 - val_acc: 0.5833\n",
      "Epoch 811/1000\n",
      "644/644 [==============================] - ETA: 0s - loss: -6.5180 - acc: 0.62 - 0s 98us/step - loss: -6.2039 - acc: 0.5776 - val_loss: -3.6306 - val_acc: 0.5833\n",
      "Epoch 812/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.2032 - acc: 0.5745 - val_loss: -3.6314 - val_acc: 0.5833\n",
      "Epoch 813/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.2036 - acc: 0.5745 - val_loss: -3.6313 - val_acc: 0.5870\n",
      "Epoch 814/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.2069 - acc: 0.5776 - val_loss: -3.6338 - val_acc: 0.5870\n",
      "Epoch 815/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.2052 - acc: 0.5761 - val_loss: -3.6311 - val_acc: 0.5870\n",
      "Epoch 816/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2079 - acc: 0.5761 - val_loss: -3.6333 - val_acc: 0.5870\n",
      "Epoch 817/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644/644 [==============================] - 0s 103us/step - loss: -6.2059 - acc: 0.5745 - val_loss: -3.6338 - val_acc: 0.5870\n",
      "Epoch 818/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.2083 - acc: 0.5745 - val_loss: -3.6312 - val_acc: 0.5870\n",
      "Epoch 819/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.2095 - acc: 0.5776 - val_loss: -3.6342 - val_acc: 0.5870\n",
      "Epoch 820/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2068 - acc: 0.5761 - val_loss: -3.6342 - val_acc: 0.5870\n",
      "Epoch 821/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.2080 - acc: 0.5745 - val_loss: -3.6352 - val_acc: 0.5870\n",
      "Epoch 822/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2107 - acc: 0.5745 - val_loss: -3.6319 - val_acc: 0.5870\n",
      "Epoch 823/1000\n",
      "644/644 [==============================] - 0s 92us/step - loss: -6.2092 - acc: 0.5761 - val_loss: -3.6348 - val_acc: 0.5870\n",
      "Epoch 824/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.2101 - acc: 0.5761 - val_loss: -3.6341 - val_acc: 0.5870\n",
      "Epoch 825/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2119 - acc: 0.5776 - val_loss: -3.6338 - val_acc: 0.5870\n",
      "Epoch 826/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.2105 - acc: 0.5745 - val_loss: -3.6354 - val_acc: 0.5870\n",
      "Epoch 827/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -6.2107 - acc: 0.5745 - val_loss: -3.6351 - val_acc: 0.5870\n",
      "Epoch 828/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.2115 - acc: 0.5761 - val_loss: -3.6346 - val_acc: 0.5870\n",
      "Epoch 829/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.2121 - acc: 0.5745 - val_loss: -3.6348 - val_acc: 0.5870\n",
      "Epoch 830/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.2124 - acc: 0.5745 - val_loss: -3.6345 - val_acc: 0.5870\n",
      "Epoch 831/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.2135 - acc: 0.5745 - val_loss: -3.6348 - val_acc: 0.5870\n",
      "Epoch 832/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.2136 - acc: 0.5761 - val_loss: -3.6343 - val_acc: 0.5870\n",
      "Epoch 833/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.2131 - acc: 0.5745 - val_loss: -3.6345 - val_acc: 0.5870\n",
      "Epoch 834/1000\n",
      "644/644 [==============================] - 0s 114us/step - loss: -6.2156 - acc: 0.5761 - val_loss: -3.6335 - val_acc: 0.5870\n",
      "Epoch 835/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -6.2169 - acc: 0.5761 - val_loss: -3.6347 - val_acc: 0.5870\n",
      "Epoch 836/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.2146 - acc: 0.5745 - val_loss: -3.6342 - val_acc: 0.5870\n",
      "Epoch 837/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2171 - acc: 0.5776 - val_loss: -3.6339 - val_acc: 0.5870\n",
      "Epoch 838/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2151 - acc: 0.5745 - val_loss: -3.6337 - val_acc: 0.5870\n",
      "Epoch 839/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.2171 - acc: 0.5761 - val_loss: -3.6339 - val_acc: 0.5870\n",
      "Epoch 840/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.2167 - acc: 0.5761 - val_loss: -3.6331 - val_acc: 0.5870\n",
      "Epoch 841/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.2163 - acc: 0.5761 - val_loss: -3.6346 - val_acc: 0.5870\n",
      "Epoch 842/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.2179 - acc: 0.5745 - val_loss: -3.6341 - val_acc: 0.5870\n",
      "Epoch 843/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.2177 - acc: 0.5745 - val_loss: -3.6347 - val_acc: 0.5870\n",
      "Epoch 844/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2196 - acc: 0.5745 - val_loss: -3.6348 - val_acc: 0.5870\n",
      "Epoch 845/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.2193 - acc: 0.5745 - val_loss: -3.6334 - val_acc: 0.5870\n",
      "Epoch 846/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.2208 - acc: 0.5761 - val_loss: -3.6338 - val_acc: 0.5870\n",
      "Epoch 847/1000\n",
      "644/644 [==============================] - 0s 113us/step - loss: -6.2201 - acc: 0.5761 - val_loss: -3.6356 - val_acc: 0.5833\n",
      "Epoch 848/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.2216 - acc: 0.5761 - val_loss: -3.6356 - val_acc: 0.5833\n",
      "Epoch 849/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.2211 - acc: 0.5761 - val_loss: -3.6357 - val_acc: 0.5870\n",
      "Epoch 850/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.2195 - acc: 0.5761 - val_loss: -3.6336 - val_acc: 0.5870\n",
      "Epoch 851/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.2231 - acc: 0.5761 - val_loss: -3.6349 - val_acc: 0.5870\n",
      "Epoch 852/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.2233 - acc: 0.5761 - val_loss: -3.6335 - val_acc: 0.5870\n",
      "Epoch 853/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -6.2214 - acc: 0.5761 - val_loss: -3.6357 - val_acc: 0.5833\n",
      "Epoch 854/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.2218 - acc: 0.5761 - val_loss: -3.6356 - val_acc: 0.5833\n",
      "Epoch 855/1000\n",
      "644/644 [==============================] - 0s 94us/step - loss: -6.2232 - acc: 0.5761 - val_loss: -3.6338 - val_acc: 0.5833\n",
      "Epoch 856/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.2232 - acc: 0.5761 - val_loss: -3.6358 - val_acc: 0.5870\n",
      "Epoch 857/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.2243 - acc: 0.5761 - val_loss: -3.6356 - val_acc: 0.5833\n",
      "Epoch 858/1000\n",
      "644/644 [==============================] - 0s 93us/step - loss: -6.2253 - acc: 0.5761 - val_loss: -3.6354 - val_acc: 0.5833\n",
      "Epoch 859/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2266 - acc: 0.5730 - val_loss: -3.6372 - val_acc: 0.5833\n",
      "Epoch 860/1000\n",
      "644/644 [==============================] - ETA: 0s - loss: -7.8673 - acc: 0.58 - 0s 104us/step - loss: -6.2246 - acc: 0.5761 - val_loss: -3.6364 - val_acc: 0.5833\n",
      "Epoch 861/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.2259 - acc: 0.5745 - val_loss: -3.6363 - val_acc: 0.5833\n",
      "Epoch 862/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2254 - acc: 0.5761 - val_loss: -3.6366 - val_acc: 0.5833\n",
      "Epoch 863/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2254 - acc: 0.5761 - val_loss: -3.6357 - val_acc: 0.5833\n",
      "Epoch 864/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.2282 - acc: 0.5761 - val_loss: -3.6365 - val_acc: 0.5833\n",
      "Epoch 865/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2258 - acc: 0.5745 - val_loss: -3.6369 - val_acc: 0.5833\n",
      "Epoch 866/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.2283 - acc: 0.5761 - val_loss: -3.6366 - val_acc: 0.5833\n",
      "Epoch 867/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.2277 - acc: 0.5761 - val_loss: -3.6372 - val_acc: 0.5833\n",
      "Epoch 868/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.2279 - acc: 0.5745 - val_loss: -3.6370 - val_acc: 0.5833\n",
      "Epoch 869/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.2294 - acc: 0.5745 - val_loss: -3.6366 - val_acc: 0.5833\n",
      "Epoch 870/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2287 - acc: 0.5761 - val_loss: -3.6368 - val_acc: 0.5833\n",
      "Epoch 871/1000\n",
      "644/644 [==============================] - 0s 91us/step - loss: -6.2284 - acc: 0.5745 - val_loss: -3.6383 - val_acc: 0.5833\n",
      "Epoch 872/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.2317 - acc: 0.5761 - val_loss: -3.6386 - val_acc: 0.5833\n",
      "Epoch 873/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.2300 - acc: 0.5745 - val_loss: -3.6382 - val_acc: 0.5833\n",
      "Epoch 874/1000\n",
      "644/644 [==============================] - ETA: 0s - loss: -8.0847 - acc: 0.52 - 0s 99us/step - loss: -6.2326 - acc: 0.5761 - val_loss: -3.6381 - val_acc: 0.5833\n",
      "Epoch 875/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644/644 [==============================] - 0s 107us/step - loss: -6.2324 - acc: 0.5745 - val_loss: -3.6379 - val_acc: 0.5833\n",
      "Epoch 876/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.2319 - acc: 0.5761 - val_loss: -3.6383 - val_acc: 0.5833\n",
      "Epoch 877/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2336 - acc: 0.5761 - val_loss: -3.6388 - val_acc: 0.5833\n",
      "Epoch 878/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.2319 - acc: 0.5745 - val_loss: -3.6386 - val_acc: 0.5833\n",
      "Epoch 879/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.2325 - acc: 0.5776 - val_loss: -3.6394 - val_acc: 0.5833\n",
      "Epoch 880/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2334 - acc: 0.5761 - val_loss: -3.6395 - val_acc: 0.5833\n",
      "Epoch 881/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.2331 - acc: 0.5776 - val_loss: -3.6407 - val_acc: 0.5833\n",
      "Epoch 882/1000\n",
      "644/644 [==============================] - 0s 120us/step - loss: -6.2344 - acc: 0.5761 - val_loss: -3.6402 - val_acc: 0.5833\n",
      "Epoch 883/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -6.2353 - acc: 0.5761 - val_loss: -3.6390 - val_acc: 0.5833\n",
      "Epoch 884/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2347 - acc: 0.5761 - val_loss: -3.6391 - val_acc: 0.5833\n",
      "Epoch 885/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.2345 - acc: 0.5761 - val_loss: -3.6395 - val_acc: 0.5833\n",
      "Epoch 886/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.2361 - acc: 0.5761 - val_loss: -3.6403 - val_acc: 0.5833\n",
      "Epoch 887/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -6.2363 - acc: 0.5761 - val_loss: -3.6401 - val_acc: 0.5833\n",
      "Epoch 888/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2387 - acc: 0.5761 - val_loss: -3.6404 - val_acc: 0.5833\n",
      "Epoch 889/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.2391 - acc: 0.5776 - val_loss: -3.6411 - val_acc: 0.5833\n",
      "Epoch 890/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.2367 - acc: 0.5761 - val_loss: -3.6419 - val_acc: 0.5833\n",
      "Epoch 891/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2386 - acc: 0.5761 - val_loss: -3.6417 - val_acc: 0.5833\n",
      "Epoch 892/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.2386 - acc: 0.5761 - val_loss: -3.6416 - val_acc: 0.5833\n",
      "Epoch 893/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.2395 - acc: 0.5761 - val_loss: -3.6418 - val_acc: 0.5833\n",
      "Epoch 894/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2399 - acc: 0.5776 - val_loss: -3.6416 - val_acc: 0.5833\n",
      "Epoch 895/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.2402 - acc: 0.5776 - val_loss: -3.6420 - val_acc: 0.5833\n",
      "Epoch 896/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2410 - acc: 0.5761 - val_loss: -3.6429 - val_acc: 0.5833\n",
      "Epoch 897/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.2419 - acc: 0.5776 - val_loss: -3.6426 - val_acc: 0.5833\n",
      "Epoch 898/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2409 - acc: 0.5761 - val_loss: -3.6435 - val_acc: 0.5797\n",
      "Epoch 899/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2437 - acc: 0.5776 - val_loss: -3.6445 - val_acc: 0.5797\n",
      "Epoch 900/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -6.2441 - acc: 0.5761 - val_loss: -3.6440 - val_acc: 0.5797\n",
      "Epoch 901/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.2444 - acc: 0.5776 - val_loss: -3.6439 - val_acc: 0.5797\n",
      "Epoch 902/1000\n",
      "644/644 [==============================] - 0s 94us/step - loss: -6.2451 - acc: 0.5776 - val_loss: -3.6438 - val_acc: 0.5797\n",
      "Epoch 903/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.2452 - acc: 0.5776 - val_loss: -3.6441 - val_acc: 0.5797\n",
      "Epoch 904/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.2458 - acc: 0.5776 - val_loss: -3.6433 - val_acc: 0.5797\n",
      "Epoch 905/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.2457 - acc: 0.5776 - val_loss: -3.6435 - val_acc: 0.5797\n",
      "Epoch 906/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2450 - acc: 0.5792 - val_loss: -3.6451 - val_acc: 0.5797\n",
      "Epoch 907/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2476 - acc: 0.5792 - val_loss: -3.6448 - val_acc: 0.5797\n",
      "Epoch 908/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.2469 - acc: 0.5776 - val_loss: -3.6453 - val_acc: 0.5797\n",
      "Epoch 909/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2460 - acc: 0.5792 - val_loss: -3.6449 - val_acc: 0.5797\n",
      "Epoch 910/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.2478 - acc: 0.5776 - val_loss: -3.6449 - val_acc: 0.5797\n",
      "Epoch 911/1000\n",
      "644/644 [==============================] - 0s 115us/step - loss: -6.2489 - acc: 0.5776 - val_loss: -3.6454 - val_acc: 0.5797\n",
      "Epoch 912/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.2492 - acc: 0.5776 - val_loss: -3.6456 - val_acc: 0.5797\n",
      "Epoch 913/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2487 - acc: 0.5792 - val_loss: -3.6455 - val_acc: 0.5797\n",
      "Epoch 914/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2480 - acc: 0.5792 - val_loss: -3.6460 - val_acc: 0.5797\n",
      "Epoch 915/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2502 - acc: 0.5792 - val_loss: -3.6461 - val_acc: 0.5797\n",
      "Epoch 916/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.2497 - acc: 0.5792 - val_loss: -3.6486 - val_acc: 0.5797\n",
      "Epoch 917/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.2495 - acc: 0.5792 - val_loss: -3.6464 - val_acc: 0.5797\n",
      "Epoch 918/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.2517 - acc: 0.5776 - val_loss: -3.6489 - val_acc: 0.5797\n",
      "Epoch 919/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -6.2510 - acc: 0.5792 - val_loss: -3.6486 - val_acc: 0.5797\n",
      "Epoch 920/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.2510 - acc: 0.5792 - val_loss: -3.6463 - val_acc: 0.5797\n",
      "Epoch 921/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -6.2513 - acc: 0.5792 - val_loss: -3.6490 - val_acc: 0.5797\n",
      "Epoch 922/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.2520 - acc: 0.5792 - val_loss: -3.6491 - val_acc: 0.5797\n",
      "Epoch 923/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.2522 - acc: 0.5792 - val_loss: -3.6524 - val_acc: 0.5797\n",
      "Epoch 924/1000\n",
      "644/644 [==============================] - 0s 93us/step - loss: -6.2515 - acc: 0.5807 - val_loss: -3.6498 - val_acc: 0.5797\n",
      "Epoch 925/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.2526 - acc: 0.5807 - val_loss: -3.6500 - val_acc: 0.5797\n",
      "Epoch 926/1000\n",
      "644/644 [==============================] - 0s 95us/step - loss: -6.2542 - acc: 0.5792 - val_loss: -3.6510 - val_acc: 0.5797\n",
      "Epoch 927/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -6.2535 - acc: 0.5807 - val_loss: -3.6518 - val_acc: 0.5797\n",
      "Epoch 928/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.2538 - acc: 0.5792 - val_loss: -3.6524 - val_acc: 0.5797\n",
      "Epoch 929/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.2539 - acc: 0.5807 - val_loss: -3.6514 - val_acc: 0.5797\n",
      "Epoch 930/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.2542 - acc: 0.5807 - val_loss: -3.6521 - val_acc: 0.5797\n",
      "Epoch 931/1000\n",
      "644/644 [==============================] - ETA: 0s - loss: -6.4376 - acc: 0.50 - 0s 100us/step - loss: -6.2539 - acc: 0.5807 - val_loss: -3.6514 - val_acc: 0.5797\n",
      "Epoch 932/1000\n",
      "644/644 [==============================] - ETA: 0s - loss: -5.6424 - acc: 0.52 - 0s 99us/step - loss: -6.2554 - acc: 0.5823 - val_loss: -3.6525 - val_acc: 0.5797\n",
      "Epoch 933/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644/644 [==============================] - 0s 109us/step - loss: -6.2558 - acc: 0.5807 - val_loss: -3.6524 - val_acc: 0.5797\n",
      "Epoch 934/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.2561 - acc: 0.5823 - val_loss: -3.6529 - val_acc: 0.5797\n",
      "Epoch 935/1000\n",
      "644/644 [==============================] - 0s 113us/step - loss: -6.2563 - acc: 0.5807 - val_loss: -3.6535 - val_acc: 0.5797\n",
      "Epoch 936/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.2560 - acc: 0.5776 - val_loss: -3.6533 - val_acc: 0.5761\n",
      "Epoch 937/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.2557 - acc: 0.5823 - val_loss: -3.6544 - val_acc: 0.5761\n",
      "Epoch 938/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.2572 - acc: 0.5823 - val_loss: -3.6543 - val_acc: 0.5761\n",
      "Epoch 939/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2585 - acc: 0.5823 - val_loss: -3.6547 - val_acc: 0.5761\n",
      "Epoch 940/1000\n",
      "644/644 [==============================] - 0s 111us/step - loss: -6.2577 - acc: 0.5807 - val_loss: -3.6550 - val_acc: 0.5797\n",
      "Epoch 941/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.2597 - acc: 0.5807 - val_loss: -3.6545 - val_acc: 0.5797\n",
      "Epoch 942/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2590 - acc: 0.5807 - val_loss: -3.6541 - val_acc: 0.5761\n",
      "Epoch 943/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.2602 - acc: 0.5823 - val_loss: -3.6545 - val_acc: 0.5761\n",
      "Epoch 944/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.2605 - acc: 0.5792 - val_loss: -3.6541 - val_acc: 0.5761\n",
      "Epoch 945/1000\n",
      "644/644 [==============================] - 0s 95us/step - loss: -6.2609 - acc: 0.5807 - val_loss: -3.6546 - val_acc: 0.5761\n",
      "Epoch 946/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -6.2607 - acc: 0.5807 - val_loss: -3.6550 - val_acc: 0.5761\n",
      "Epoch 947/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2602 - acc: 0.5792 - val_loss: -3.6558 - val_acc: 0.5761\n",
      "Epoch 948/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.2605 - acc: 0.5792 - val_loss: -3.6561 - val_acc: 0.5761\n",
      "Epoch 949/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2622 - acc: 0.5823 - val_loss: -3.6549 - val_acc: 0.5761\n",
      "Epoch 950/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2619 - acc: 0.5823 - val_loss: -3.6556 - val_acc: 0.5761\n",
      "Epoch 951/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.2615 - acc: 0.5823 - val_loss: -3.6558 - val_acc: 0.5761\n",
      "Epoch 952/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.2621 - acc: 0.5823 - val_loss: -3.6565 - val_acc: 0.5761\n",
      "Epoch 953/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2631 - acc: 0.5807 - val_loss: -3.6561 - val_acc: 0.5761\n",
      "Epoch 954/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2625 - acc: 0.5807 - val_loss: -3.6582 - val_acc: 0.5761\n",
      "Epoch 955/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.2660 - acc: 0.5807 - val_loss: -3.6556 - val_acc: 0.5761\n",
      "Epoch 956/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.2639 - acc: 0.5823 - val_loss: -3.6578 - val_acc: 0.5761\n",
      "Epoch 957/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.2644 - acc: 0.5839 - val_loss: -3.6568 - val_acc: 0.5761\n",
      "Epoch 958/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.2654 - acc: 0.5839 - val_loss: -3.6570 - val_acc: 0.5761\n",
      "Epoch 959/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2663 - acc: 0.5823 - val_loss: -3.6577 - val_acc: 0.5761\n",
      "Epoch 960/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.2668 - acc: 0.5807 - val_loss: -3.6585 - val_acc: 0.5761\n",
      "Epoch 961/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2664 - acc: 0.5823 - val_loss: -3.6570 - val_acc: 0.5761\n",
      "Epoch 962/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.2663 - acc: 0.5839 - val_loss: -3.6570 - val_acc: 0.5761\n",
      "Epoch 963/1000\n",
      "644/644 [==============================] - 0s 109us/step - loss: -6.2661 - acc: 0.5854 - val_loss: -3.6567 - val_acc: 0.5761\n",
      "Epoch 964/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.2673 - acc: 0.5854 - val_loss: -3.6575 - val_acc: 0.5761\n",
      "Epoch 965/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.2675 - acc: 0.5854 - val_loss: -3.6584 - val_acc: 0.5761\n",
      "Epoch 966/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.2670 - acc: 0.5839 - val_loss: -3.6591 - val_acc: 0.5761\n",
      "Epoch 967/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2698 - acc: 0.5823 - val_loss: -3.6582 - val_acc: 0.5761\n",
      "Epoch 968/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -6.2665 - acc: 0.5807 - val_loss: -3.6591 - val_acc: 0.5761\n",
      "Epoch 969/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.2686 - acc: 0.5839 - val_loss: -3.6598 - val_acc: 0.5761\n",
      "Epoch 970/1000\n",
      "644/644 [==============================] - 0s 113us/step - loss: -6.2682 - acc: 0.5823 - val_loss: -3.6599 - val_acc: 0.5761\n",
      "Epoch 971/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.2686 - acc: 0.5823 - val_loss: -3.6596 - val_acc: 0.5761\n",
      "Epoch 972/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -6.2706 - acc: 0.5854 - val_loss: -3.6591 - val_acc: 0.5761\n",
      "Epoch 973/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.2695 - acc: 0.5839 - val_loss: -3.6602 - val_acc: 0.5761\n",
      "Epoch 974/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.2704 - acc: 0.5839 - val_loss: -3.6600 - val_acc: 0.5761\n",
      "Epoch 975/1000\n",
      "644/644 [==============================] - 0s 96us/step - loss: -6.2709 - acc: 0.5854 - val_loss: -3.6601 - val_acc: 0.5761\n",
      "Epoch 976/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.2709 - acc: 0.5839 - val_loss: -3.6608 - val_acc: 0.5761\n",
      "Epoch 977/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.2709 - acc: 0.5854 - val_loss: -3.6618 - val_acc: 0.5761\n",
      "Epoch 978/1000\n",
      "644/644 [==============================] - 0s 99us/step - loss: -6.2723 - acc: 0.5823 - val_loss: -3.6611 - val_acc: 0.5761\n",
      "Epoch 979/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.2718 - acc: 0.5823 - val_loss: -3.6626 - val_acc: 0.5761\n",
      "Epoch 980/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2721 - acc: 0.5839 - val_loss: -3.6624 - val_acc: 0.5761\n",
      "Epoch 981/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.2725 - acc: 0.5839 - val_loss: -3.6625 - val_acc: 0.5761\n",
      "Epoch 982/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2726 - acc: 0.5854 - val_loss: -3.6624 - val_acc: 0.5761\n",
      "Epoch 983/1000\n",
      "644/644 [==============================] - 0s 108us/step - loss: -6.2736 - acc: 0.5839 - val_loss: -3.6619 - val_acc: 0.5761\n",
      "Epoch 984/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.2731 - acc: 0.5839 - val_loss: -3.6630 - val_acc: 0.5761\n",
      "Epoch 985/1000\n",
      "644/644 [==============================] - 0s 97us/step - loss: -6.2732 - acc: 0.5839 - val_loss: -3.6625 - val_acc: 0.5761\n",
      "Epoch 986/1000\n",
      "644/644 [==============================] - 0s 101us/step - loss: -6.2744 - acc: 0.5823 - val_loss: -3.6619 - val_acc: 0.5761\n",
      "Epoch 987/1000\n",
      "644/644 [==============================] - ETA: 0s - loss: -7.0857 - acc: 0.54 - 0s 101us/step - loss: -6.2753 - acc: 0.5823 - val_loss: -3.6628 - val_acc: 0.5761\n",
      "Epoch 988/1000\n",
      "644/644 [==============================] - 0s 106us/step - loss: -6.2754 - acc: 0.5823 - val_loss: -3.6625 - val_acc: 0.5761\n",
      "Epoch 989/1000\n",
      "644/644 [==============================] - 0s 103us/step - loss: -6.2753 - acc: 0.5839 - val_loss: -3.6622 - val_acc: 0.5761\n",
      "Epoch 990/1000\n",
      "644/644 [==============================] - 0s 104us/step - loss: -6.2761 - acc: 0.5823 - val_loss: -3.6625 - val_acc: 0.5761\n",
      "Epoch 991/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644/644 [==============================] - 0s 101us/step - loss: -6.2773 - acc: 0.5854 - val_loss: -3.6628 - val_acc: 0.5761\n",
      "Epoch 992/1000\n",
      "644/644 [==============================] - 0s 100us/step - loss: -6.2768 - acc: 0.5839 - val_loss: -3.6641 - val_acc: 0.5761\n",
      "Epoch 993/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.2772 - acc: 0.5839 - val_loss: -3.6661 - val_acc: 0.5761\n",
      "Epoch 994/1000\n",
      "644/644 [==============================] - 0s 110us/step - loss: -6.2778 - acc: 0.5839 - val_loss: -3.6644 - val_acc: 0.5761\n",
      "Epoch 995/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.2782 - acc: 0.5823 - val_loss: -3.6644 - val_acc: 0.5761\n",
      "Epoch 996/1000\n",
      "644/644 [==============================] - 0s 107us/step - loss: -6.2781 - acc: 0.5823 - val_loss: -3.6653 - val_acc: 0.5761\n",
      "Epoch 997/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.2787 - acc: 0.5839 - val_loss: -3.6657 - val_acc: 0.5761\n",
      "Epoch 998/1000\n",
      "644/644 [==============================] - 0s 98us/step - loss: -6.2800 - acc: 0.5839 - val_loss: -3.6665 - val_acc: 0.5761\n",
      "Epoch 999/1000\n",
      "644/644 [==============================] - 0s 102us/step - loss: -6.2795 - acc: 0.5823 - val_loss: -3.6653 - val_acc: 0.5761\n",
      "Epoch 1000/1000\n",
      "644/644 [==============================] - 0s 105us/step - loss: -6.2804 - acc: 0.5854 - val_loss: -3.6658 - val_acc: 0.5761\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train, epochs=1000, batch_size=50, validation_data=(x_test,y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noG+MMQXEgr4xxhSQ/w9SSp8Ab3RnsAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2977c8699e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEICAYAAABLdt/UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2977ca261d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = hist.history['acc']\n",
    "val_acc = hist.history['val_acc']\n",
    "loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.77261901,  0.75498736,  0.26368928,  0.66271484, -0.02421921,\n",
       "          1.51375651,  0.70426148,  0.04418723,  0.02798159, -0.47818539],\n",
       "        [ 1.41559124,  1.14065695,  0.5225637 ,  1.41689563,  0.67664671,\n",
       "          2.36581612, -0.885759  ,  0.9488312 ,  0.55638731,  0.45756996],\n",
       "        [ 0.58797783,  0.86125231, -0.81730402,  1.03017676,  0.54075861,\n",
       "          1.10011673, -0.06668501,  1.21754813, -1.0464015 , -0.7842226 ],\n",
       "        [-0.23636433,  0.23399679, -0.70127016,  0.09249299,  0.02731962,\n",
       "          0.26038435, -0.05839209, -1.00386763, -0.08111461, -1.00063348],\n",
       "        [-1.71837592, -3.36770129,  1.26959312,  0.20018317,  0.16792375,\n",
       "         -1.43369067, -0.15665069, -0.38282067, -0.13628627,  0.10103836],\n",
       "        [ 0.5637942 ,  0.71481681, -1.62643063,  0.16614887, -0.82361102,\n",
       "          0.6959278 , -1.09779072, -0.04714641,  0.23825592, -0.36877134],\n",
       "        [-0.66013086, -0.47656545, -1.01514053,  0.53283036, -0.43820804,\n",
       "         -0.47345325, -0.22984549, -0.2047399 , -0.51399481,  0.63390988],\n",
       "        [-1.20362651, -1.46537781, -1.24782145,  0.57574993, -1.37801933,\n",
       "         -2.09860992, -1.43536961,  0.02370143,  0.53680909,  0.10325713],\n",
       "        [ 1.07803655,  0.51672715,  0.33829272,  0.86558431, -1.86060572,\n",
       "          1.81542039, -0.65445662,  1.00520992, -0.61696434, -0.6293807 ],\n",
       "        [ 2.046736  ,  1.36405551, -1.79798782,  1.85258663, -0.90038311,\n",
       "          1.50118113,  0.15338778,  1.07238197, -0.86251885,  0.1499624 ],\n",
       "        [-0.11777737,  0.20574112,  0.42801854, -0.38403174,  0.58695787,\n",
       "          0.28872123, -0.72693896, -0.08286536, -0.39391029,  0.52801645],\n",
       "        [ 1.93102121,  1.52918816,  0.13849883,  1.50726449,  0.66069442,\n",
       "          2.33918381,  0.32976931,  1.50842333, -0.90083921, -1.17656815],\n",
       "        [ 1.34375942,  0.41705847, -0.13660268,  2.9642756 ,  0.37719968,\n",
       "          1.0745225 ,  1.10301495,  2.69436646,  0.5444442 , -0.29610369]], dtype=float32),\n",
       " array([ 0.19396037,  0.67110139, -0.37593043,  0.78885138, -0.46272433,\n",
       "         0.05482923,  0.65776134,  0.55875599,  0.58746535,  0.62811035], dtype=float32),\n",
       " array([[ 3.67935514],\n",
       "        [ 3.49786234],\n",
       "        [ 2.38736463],\n",
       "        [ 2.53579378],\n",
       "        [-2.02063823],\n",
       "        [ 3.85284853],\n",
       "        [-2.54354191],\n",
       "        [ 2.53288841],\n",
       "        [-1.42767537],\n",
       "        [-1.19064724]], dtype=float32),\n",
       " array([ 0.94822145], dtype=float32)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
